{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML2021Spring - HW1_r09921006.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvin870203/ntuml21s/blob/main/ML2021Spring_HW1_r09921006.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeZnPAiwDRWG"
      },
      "source": [
        "Author: Chih-Yuan Chuang (r09921006)\n",
        "\n",
        "Slides: https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.pdf  \n",
        "Video: TBA\n",
        "\n",
        "Objectives:\n",
        "* Solve a regression problem with deep neural networks (DNN).\n",
        "* Understand basic DNN training tips.\n",
        "* Get familiar with PyTorch.\n",
        "\n",
        "If any questions, please contact the TAs via TA hours, NTU COOL, or email.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx3x1nDkG-Uy"
      },
      "source": [
        "# **Download Data**\n",
        "\n",
        "\n",
        "If the Google drive links are dead, you can download data from [kaggle](https://www.kaggle.com/c/ml2021spring-hw1/data), and upload data manually to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMj55YDKG6ch",
        "outputId": "6dae67d2-4d67-424c-8ff0-0ec5bb34f2e6"
      },
      "source": [
        "tr_path = 'covid.train.csv'  # path to training data\n",
        "tt_path = 'covid.test.csv'   # path to testing data\n",
        "\n",
        "!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
        "!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 31.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 10.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_4-77xHk44"
      },
      "source": [
        "# **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-onQd4JNA5H"
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "myseed = 42069  # set a random seed for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtE3b6JEH7rw"
      },
      "source": [
        "# **Some Utilities**\n",
        "\n",
        "You do not need to modify this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWMT3uf1NGQp"
      },
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])\n",
        "    x_1 = range(total_steps)\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
        "    plt.ylim(0.0, 5.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "    ''' Plot prediction of your DNN '''\n",
        "    if preds is None or targets is None:\n",
        "        model.eval()\n",
        "        preds, targets = [], []\n",
        "        for x, y in dv_set:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = model(x)\n",
        "                preds.append(pred.detach().cpu())\n",
        "                targets.append(y.detach().cpu())\n",
        "        preds = torch.cat(preds, dim=0).numpy()\n",
        "        targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "    figure(figsize=(5, 5))\n",
        "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "    plt.xlim(-0.2, lim)\n",
        "    plt.ylim(-0.2, lim)\n",
        "    plt.xlabel('ground truth value')\n",
        "    plt.ylabel('predicted value')\n",
        "    plt.title('Ground Truth v.s. Prediction')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39U_XFX6KOoj"
      },
      "source": [
        "# **Preprocess**\n",
        "\n",
        "We have three kinds of datasets:\n",
        "* `train`: for training\n",
        "* `dev`: for validation\n",
        "* `test`: for testing (w/o target value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ-MdwpLL7Dt"
      },
      "source": [
        "## **Dataset**\n",
        "\n",
        "The `COVID19Dataset` below does:\n",
        "* read `.csv` files\n",
        "* extract features\n",
        "* split `covid.train.csv` into train/dev sets\n",
        "* normalize features\n",
        "\n",
        "Finishing `TODO` below might make you pass medium baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNGUwJep9Tj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e8e8cf2-9004-4ddc-a8fa-0bf30b3ebcac"
      },
      "source": [
        "split_num = 1\n",
        "with open(tr_path, 'r') as fp:\n",
        "    data = list(csv.reader(fp))\n",
        "    data = np.array(data[1:])[:, 1:].astype(float)\n",
        "    feats = [40,41,42,43] + [58,59,60,61] + [76,77,78,79] + [57, 75]\n",
        "    data = data[:, feats]\n",
        "    indices = [i for i in range(len(data)) if i % 10 != split_num]\n",
        "    data = data[indices]\n",
        "    MEAN = np.mean(data, axis=0, keepdims=True)\n",
        "    STD = np.std(data, axis=0, keepdims=True)\n",
        "print(MEAN, STD)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.99126059  1.01597479 29.4418313  24.32465294  0.99416455  1.01903335\n",
            "  29.53015039 24.40694143  0.99759178  1.02212743 29.6130458  24.47952089\n",
            "  16.30043802 16.37600015]] [[0.42021428 0.42362659 9.10470848 8.45788584 0.42034576 0.42362107\n",
            "  9.09257428 8.45543431 0.42034031 0.42352935 9.08173029 8.44968563\n",
            "  7.64451925 7.63073901]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zlpIp9ANJRU"
      },
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "    def __init__(self,\n",
        "                 path,\n",
        "                 mode='train',\n",
        "                 target_only=False):\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\n",
        "        \n",
        "        if not target_only:\n",
        "            feats = list(range(93))\n",
        "        else:\n",
        "            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\n",
        "            feats = list(range(40)) + [40,41,42,43] + [58,59,60,61] + [76,77,78,79] + [57, 75]\n",
        "            pass\n",
        "\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            self.data = torch.FloatTensor(data)\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            data = data[:, feats]\n",
        "            \n",
        "            # Splitting training data into train & dev sets\n",
        "            if mode == 'train':\n",
        "                indices = [i for i in range(len(data)) if i % 10 != split_num]\n",
        "            elif mode == 'dev':\n",
        "                indices = [i for i in range(len(data)) if i % 10 == split_num]\n",
        "            \n",
        "            # Convert data into PyTorch tensors\n",
        "            self.data = torch.FloatTensor(data[indices])\n",
        "            self.target = torch.FloatTensor(target[indices])\n",
        "\n",
        "        MEAN\n",
        "\n",
        "        # Normalize features (you may remove this part to see what will happen)\n",
        "        self.data[:, 40:] = \\\n",
        "            (self.data[:, 40:] - torch.FloatTensor(MEAN)) \\\n",
        "            / torch.FloatTensor(STD)\n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhTlkE7MDo3"
      },
      "source": [
        "## **DataLoader**\n",
        "\n",
        "A `DataLoader` loads data from a given `Dataset` into batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlhLk5t6MBX3"
      },
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuycwR0MeQB"
      },
      "source": [
        "# **Deep Neural Network**\n",
        "\n",
        "`NeuralNet` is an `nn.Module` designed for regression.\n",
        "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
        "This module also included a function `cal_loss` for calculating loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49-uXYovOAI0"
      },
      "source": [
        "l2_lambda = 0.01\n",
        "class NeuralNet(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(input_dim, input_dim),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(input_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        # TODO: you may implement L2 regularization here\n",
        "        L2 = 0\n",
        "        for param in self.parameters():\n",
        "            L2 += torch.sum(torch.pow(param, 2))\n",
        "        return self.criterion(pred, target) + l2_lambda * L2\n",
        "        # return self.criterion(pred, target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvFWVjZ5Nvga"
      },
      "source": [
        "# **Train/Dev/Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAM8QecJOyqn"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOqcmYzMO7jB"
      },
      "source": [
        "def train(tr_set, dv_set, model, config, device):\n",
        "    ''' DNN training '''\n",
        "\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "    \n",
        "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10)\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\n",
        "            optimizer.zero_grad()               # set gradient to zero\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "            optimizer.step()                    # update model with optimizer\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "            #scheduler.step()\n",
        "\n",
        "\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            print('Saving model (epoch = {:4d}, val loss = {:.4f}, train loss = {:.4f})'\n",
        "                .format(epoch + 1, min_mse, mse_loss))\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        epoch += 1\n",
        "        loss_record['dev'].append(dev_mse)\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    print('Final: (epoch = {:4d}, val loss = {:.4f}, train loss = {:.4f})'\n",
        "                .format(epoch + 1, min_mse, mse_loss))\n",
        "    \n",
        "    L2 = 0\n",
        "    for param in model.parameters():\n",
        "        L2 += torch.sum(torch.pow(param, 2))\n",
        "    print('Final: (epoch = {:4d}, val loss = {:.4f}, train loss = {:.4f})'\n",
        "                .format(epoch + 1, min_mse - l2_lambda*L2, mse_loss - l2_lambda*L2))\n",
        "\n",
        "    return min_mse, loss_record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hSd4Bn3O2PL"
      },
      "source": [
        "## **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrxrD3YsN3U2"
      },
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        #x[:,40:] = (x[:,40:] - MEAN) / STD\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0pdrhQAO41L"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSBMRFlYN5tB"
      },
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        #x[:,40:] = (x[:,40:] - MEAN) / STD\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvckkF5dvf0j"
      },
      "source": [
        "# **Setup Hyper-parameters**\n",
        "\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPXpdumwPjE7"
      },
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "target_only = True                    # TODO: Using 40 states & 2 tested_positive features\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "config = {\n",
        "    'n_epochs': 8000,#3000,                # maximum number of epochs\n",
        "    'batch_size': 270,#270,               # mini-batch size for dataloader\n",
        "    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.0001, #0.001,                # learning rate of SGD\n",
        "        'momentum': 0.9#,              # momentum for SGD\n",
        "        # 'weight_decay': 0.01\n",
        "    },\n",
        "    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j1eOV3TOH-j"
      },
      "source": [
        "# **Load data and model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNrYBMmePLKm",
        "outputId": "47dd24c8-9cbe-40e4-d53b-48a443e77690"
      },
      "source": [
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 54)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 54)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 54)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHylSirLP9oh"
      },
      "source": [
        "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2B_zgSOPTJ"
      },
      "source": [
        "# **Start Training!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrEbUxazQAAZ",
        "outputId": "189a8ea0-90c1-4bbb-8910-75ffcb869e88"
      },
      "source": [
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model (epoch =    1, val loss = 312.1792, train loss = 309.9084)\n",
            "Saving model (epoch =    2, val loss = 287.5938, train loss = 279.8418)\n",
            "Saving model (epoch =    3, val loss = 245.7852, train loss = 260.1877)\n",
            "Saving model (epoch =    4, val loss = 180.4177, train loss = 187.3921)\n",
            "Saving model (epoch =    5, val loss = 112.2003, train loss = 113.1466)\n",
            "Saving model (epoch =    6, val loss = 87.5495, train loss = 87.4367)\n",
            "Saving model (epoch =    7, val loss = 84.9659, train loss = 93.8092)\n",
            "Saving model (epoch =    8, val loss = 75.2054, train loss = 80.0257)\n",
            "Saving model (epoch =    9, val loss = 68.1154, train loss = 68.7044)\n",
            "Saving model (epoch =   10, val loss = 62.3349, train loss = 62.1886)\n",
            "Saving model (epoch =   11, val loss = 56.8314, train loss = 63.1249)\n",
            "Saving model (epoch =   12, val loss = 52.0081, train loss = 55.6150)\n",
            "Saving model (epoch =   13, val loss = 47.8317, train loss = 49.8691)\n",
            "Saving model (epoch =   14, val loss = 44.0765, train loss = 47.7111)\n",
            "Saving model (epoch =   15, val loss = 40.4497, train loss = 40.1284)\n",
            "Saving model (epoch =   16, val loss = 36.8636, train loss = 41.4850)\n",
            "Saving model (epoch =   17, val loss = 33.2354, train loss = 33.0569)\n",
            "Saving model (epoch =   18, val loss = 29.6742, train loss = 34.0893)\n",
            "Saving model (epoch =   19, val loss = 26.2165, train loss = 26.9481)\n",
            "Saving model (epoch =   20, val loss = 22.9274, train loss = 24.2707)\n",
            "Saving model (epoch =   21, val loss = 19.7624, train loss = 20.8210)\n",
            "Saving model (epoch =   22, val loss = 16.8052, train loss = 16.8546)\n",
            "Saving model (epoch =   23, val loss = 14.0999, train loss = 13.9012)\n",
            "Saving model (epoch =   24, val loss = 11.7814, train loss = 11.9439)\n",
            "Saving model (epoch =   25, val loss = 9.8792, train loss = 10.7390)\n",
            "Saving model (epoch =   26, val loss = 8.3166, train loss = 8.8956)\n",
            "Saving model (epoch =   27, val loss = 7.0615, train loss = 6.9850)\n",
            "Saving model (epoch =   28, val loss = 6.1247, train loss = 7.2117)\n",
            "Saving model (epoch =   29, val loss = 5.3928, train loss = 6.0961)\n",
            "Saving model (epoch =   30, val loss = 4.8263, train loss = 5.7264)\n",
            "Saving model (epoch =   31, val loss = 4.3676, train loss = 3.7894)\n",
            "Saving model (epoch =   32, val loss = 4.0153, train loss = 4.2769)\n",
            "Saving model (epoch =   33, val loss = 3.7269, train loss = 3.8797)\n",
            "Saving model (epoch =   34, val loss = 3.4886, train loss = 3.3935)\n",
            "Saving model (epoch =   35, val loss = 3.2861, train loss = 2.9401)\n",
            "Saving model (epoch =   36, val loss = 3.1108, train loss = 2.7830)\n",
            "Saving model (epoch =   37, val loss = 2.9594, train loss = 2.7956)\n",
            "Saving model (epoch =   38, val loss = 2.8325, train loss = 3.1778)\n",
            "Saving model (epoch =   39, val loss = 2.7234, train loss = 2.5922)\n",
            "Saving model (epoch =   40, val loss = 2.6296, train loss = 2.3988)\n",
            "Saving model (epoch =   41, val loss = 2.5473, train loss = 2.5842)\n",
            "Saving model (epoch =   42, val loss = 2.4749, train loss = 2.1434)\n",
            "Saving model (epoch =   43, val loss = 2.4128, train loss = 2.1292)\n",
            "Saving model (epoch =   44, val loss = 2.3572, train loss = 2.2870)\n",
            "Saving model (epoch =   45, val loss = 2.3085, train loss = 2.3220)\n",
            "Saving model (epoch =   46, val loss = 2.2657, train loss = 1.9165)\n",
            "Saving model (epoch =   47, val loss = 2.2283, train loss = 1.9720)\n",
            "Saving model (epoch =   48, val loss = 2.1966, train loss = 1.9714)\n",
            "Saving model (epoch =   49, val loss = 2.1693, train loss = 1.8370)\n",
            "Saving model (epoch =   50, val loss = 2.1465, train loss = 2.1014)\n",
            "Saving model (epoch =   51, val loss = 2.1287, train loss = 1.9783)\n",
            "Saving model (epoch =   52, val loss = 2.1106, train loss = 1.8959)\n",
            "Saving model (epoch =   53, val loss = 2.0961, train loss = 1.8515)\n",
            "Saving model (epoch =   54, val loss = 2.0826, train loss = 1.7888)\n",
            "Saving model (epoch =   55, val loss = 2.0711, train loss = 1.7027)\n",
            "Saving model (epoch =   56, val loss = 2.0607, train loss = 1.6033)\n",
            "Saving model (epoch =   57, val loss = 2.0517, train loss = 1.6093)\n",
            "Saving model (epoch =   58, val loss = 2.0430, train loss = 2.0796)\n",
            "Saving model (epoch =   59, val loss = 2.0349, train loss = 1.7569)\n",
            "Saving model (epoch =   60, val loss = 2.0277, train loss = 1.9162)\n",
            "Saving model (epoch =   61, val loss = 2.0208, train loss = 1.6014)\n",
            "Saving model (epoch =   62, val loss = 2.0144, train loss = 1.7807)\n",
            "Saving model (epoch =   63, val loss = 2.0087, train loss = 1.8146)\n",
            "Saving model (epoch =   64, val loss = 2.0028, train loss = 1.9121)\n",
            "Saving model (epoch =   65, val loss = 1.9984, train loss = 2.0006)\n",
            "Saving model (epoch =   66, val loss = 1.9923, train loss = 1.7893)\n",
            "Saving model (epoch =   67, val loss = 1.9873, train loss = 1.7484)\n",
            "Saving model (epoch =   68, val loss = 1.9828, train loss = 1.9511)\n",
            "Saving model (epoch =   69, val loss = 1.9785, train loss = 1.8219)\n",
            "Saving model (epoch =   70, val loss = 1.9745, train loss = 1.7295)\n",
            "Saving model (epoch =   71, val loss = 1.9705, train loss = 1.9662)\n",
            "Saving model (epoch =   72, val loss = 1.9667, train loss = 1.7356)\n",
            "Saving model (epoch =   73, val loss = 1.9628, train loss = 1.6172)\n",
            "Saving model (epoch =   74, val loss = 1.9590, train loss = 1.7278)\n",
            "Saving model (epoch =   75, val loss = 1.9553, train loss = 1.7508)\n",
            "Saving model (epoch =   76, val loss = 1.9519, train loss = 1.7414)\n",
            "Saving model (epoch =   77, val loss = 1.9486, train loss = 1.5573)\n",
            "Saving model (epoch =   78, val loss = 1.9453, train loss = 1.6112)\n",
            "Saving model (epoch =   79, val loss = 1.9425, train loss = 1.7058)\n",
            "Saving model (epoch =   80, val loss = 1.9399, train loss = 1.5550)\n",
            "Saving model (epoch =   81, val loss = 1.9362, train loss = 1.7239)\n",
            "Saving model (epoch =   82, val loss = 1.9336, train loss = 1.5962)\n",
            "Saving model (epoch =   83, val loss = 1.9308, train loss = 1.6736)\n",
            "Saving model (epoch =   84, val loss = 1.9281, train loss = 1.6317)\n",
            "Saving model (epoch =   85, val loss = 1.9253, train loss = 1.8549)\n",
            "Saving model (epoch =   86, val loss = 1.9231, train loss = 1.8669)\n",
            "Saving model (epoch =   87, val loss = 1.9202, train loss = 1.8122)\n",
            "Saving model (epoch =   88, val loss = 1.9174, train loss = 1.7080)\n",
            "Saving model (epoch =   89, val loss = 1.9151, train loss = 1.6823)\n",
            "Saving model (epoch =   90, val loss = 1.9125, train loss = 1.7513)\n",
            "Saving model (epoch =   91, val loss = 1.9102, train loss = 1.6979)\n",
            "Saving model (epoch =   92, val loss = 1.9078, train loss = 1.6037)\n",
            "Saving model (epoch =   93, val loss = 1.9059, train loss = 1.6194)\n",
            "Saving model (epoch =   94, val loss = 1.9042, train loss = 1.8141)\n",
            "Saving model (epoch =   95, val loss = 1.9017, train loss = 1.5823)\n",
            "Saving model (epoch =   96, val loss = 1.8997, train loss = 1.5464)\n",
            "Saving model (epoch =   97, val loss = 1.8972, train loss = 1.6800)\n",
            "Saving model (epoch =   98, val loss = 1.8950, train loss = 1.4750)\n",
            "Saving model (epoch =   99, val loss = 1.8930, train loss = 1.8061)\n",
            "Saving model (epoch =  100, val loss = 1.8915, train loss = 1.6539)\n",
            "Saving model (epoch =  101, val loss = 1.8898, train loss = 1.6201)\n",
            "Saving model (epoch =  102, val loss = 1.8875, train loss = 1.8713)\n",
            "Saving model (epoch =  103, val loss = 1.8851, train loss = 1.5953)\n",
            "Saving model (epoch =  104, val loss = 1.8837, train loss = 1.6257)\n",
            "Saving model (epoch =  105, val loss = 1.8813, train loss = 1.5102)\n",
            "Saving model (epoch =  106, val loss = 1.8797, train loss = 1.6802)\n",
            "Saving model (epoch =  107, val loss = 1.8776, train loss = 1.7294)\n",
            "Saving model (epoch =  108, val loss = 1.8758, train loss = 1.7445)\n",
            "Saving model (epoch =  109, val loss = 1.8741, train loss = 1.9742)\n",
            "Saving model (epoch =  110, val loss = 1.8726, train loss = 1.4144)\n",
            "Saving model (epoch =  111, val loss = 1.8711, train loss = 1.5786)\n",
            "Saving model (epoch =  112, val loss = 1.8696, train loss = 1.7504)\n",
            "Saving model (epoch =  113, val loss = 1.8672, train loss = 1.4725)\n",
            "Saving model (epoch =  114, val loss = 1.8656, train loss = 1.5638)\n",
            "Saving model (epoch =  115, val loss = 1.8634, train loss = 1.4163)\n",
            "Saving model (epoch =  116, val loss = 1.8630, train loss = 1.5976)\n",
            "Saving model (epoch =  117, val loss = 1.8609, train loss = 1.7512)\n",
            "Saving model (epoch =  118, val loss = 1.8587, train loss = 1.8316)\n",
            "Saving model (epoch =  119, val loss = 1.8580, train loss = 1.8529)\n",
            "Saving model (epoch =  120, val loss = 1.8559, train loss = 1.6544)\n",
            "Saving model (epoch =  121, val loss = 1.8538, train loss = 1.7505)\n",
            "Saving model (epoch =  122, val loss = 1.8522, train loss = 1.6501)\n",
            "Saving model (epoch =  123, val loss = 1.8506, train loss = 1.5295)\n",
            "Saving model (epoch =  124, val loss = 1.8495, train loss = 1.7463)\n",
            "Saving model (epoch =  125, val loss = 1.8476, train loss = 1.5187)\n",
            "Saving model (epoch =  126, val loss = 1.8460, train loss = 1.5684)\n",
            "Saving model (epoch =  127, val loss = 1.8450, train loss = 1.3796)\n",
            "Saving model (epoch =  128, val loss = 1.8432, train loss = 1.8648)\n",
            "Saving model (epoch =  129, val loss = 1.8417, train loss = 1.7752)\n",
            "Saving model (epoch =  130, val loss = 1.8400, train loss = 1.8004)\n",
            "Saving model (epoch =  131, val loss = 1.8387, train loss = 1.7211)\n",
            "Saving model (epoch =  132, val loss = 1.8376, train loss = 1.6105)\n",
            "Saving model (epoch =  133, val loss = 1.8358, train loss = 1.5421)\n",
            "Saving model (epoch =  134, val loss = 1.8345, train loss = 1.5060)\n",
            "Saving model (epoch =  135, val loss = 1.8328, train loss = 1.6962)\n",
            "Saving model (epoch =  136, val loss = 1.8316, train loss = 1.6127)\n",
            "Saving model (epoch =  137, val loss = 1.8301, train loss = 1.5313)\n",
            "Saving model (epoch =  138, val loss = 1.8290, train loss = 1.6035)\n",
            "Saving model (epoch =  139, val loss = 1.8274, train loss = 1.7598)\n",
            "Saving model (epoch =  140, val loss = 1.8256, train loss = 1.7209)\n",
            "Saving model (epoch =  141, val loss = 1.8252, train loss = 1.4509)\n",
            "Saving model (epoch =  142, val loss = 1.8230, train loss = 1.5866)\n",
            "Saving model (epoch =  143, val loss = 1.8219, train loss = 1.7516)\n",
            "Saving model (epoch =  144, val loss = 1.8205, train loss = 1.5834)\n",
            "Saving model (epoch =  145, val loss = 1.8191, train loss = 1.5877)\n",
            "Saving model (epoch =  146, val loss = 1.8179, train loss = 1.5335)\n",
            "Saving model (epoch =  147, val loss = 1.8164, train loss = 1.6221)\n",
            "Saving model (epoch =  148, val loss = 1.8151, train loss = 1.6043)\n",
            "Saving model (epoch =  149, val loss = 1.8142, train loss = 1.4816)\n",
            "Saving model (epoch =  150, val loss = 1.8125, train loss = 1.6110)\n",
            "Saving model (epoch =  151, val loss = 1.8114, train loss = 1.4944)\n",
            "Saving model (epoch =  152, val loss = 1.8103, train loss = 1.6404)\n",
            "Saving model (epoch =  153, val loss = 1.8092, train loss = 1.7675)\n",
            "Saving model (epoch =  154, val loss = 1.8082, train loss = 1.5102)\n",
            "Saving model (epoch =  155, val loss = 1.8065, train loss = 1.5400)\n",
            "Saving model (epoch =  156, val loss = 1.8053, train loss = 1.6422)\n",
            "Saving model (epoch =  157, val loss = 1.8038, train loss = 1.6654)\n",
            "Saving model (epoch =  158, val loss = 1.8027, train loss = 1.7319)\n",
            "Saving model (epoch =  159, val loss = 1.8013, train loss = 1.7160)\n",
            "Saving model (epoch =  160, val loss = 1.8003, train loss = 1.5145)\n",
            "Saving model (epoch =  161, val loss = 1.7993, train loss = 1.5982)\n",
            "Saving model (epoch =  162, val loss = 1.7979, train loss = 1.4044)\n",
            "Saving model (epoch =  163, val loss = 1.7967, train loss = 1.5700)\n",
            "Saving model (epoch =  164, val loss = 1.7956, train loss = 1.8226)\n",
            "Saving model (epoch =  165, val loss = 1.7946, train loss = 1.7497)\n",
            "Saving model (epoch =  166, val loss = 1.7932, train loss = 1.5399)\n",
            "Saving model (epoch =  167, val loss = 1.7919, train loss = 1.4477)\n",
            "Saving model (epoch =  168, val loss = 1.7910, train loss = 1.6468)\n",
            "Saving model (epoch =  169, val loss = 1.7893, train loss = 1.5095)\n",
            "Saving model (epoch =  170, val loss = 1.7884, train loss = 1.5823)\n",
            "Saving model (epoch =  171, val loss = 1.7880, train loss = 1.4866)\n",
            "Saving model (epoch =  172, val loss = 1.7861, train loss = 1.5345)\n",
            "Saving model (epoch =  173, val loss = 1.7853, train loss = 1.5064)\n",
            "Saving model (epoch =  174, val loss = 1.7840, train loss = 1.3572)\n",
            "Saving model (epoch =  175, val loss = 1.7830, train loss = 1.6252)\n",
            "Saving model (epoch =  176, val loss = 1.7820, train loss = 1.6463)\n",
            "Saving model (epoch =  177, val loss = 1.7812, train loss = 1.6413)\n",
            "Saving model (epoch =  178, val loss = 1.7803, train loss = 1.4108)\n",
            "Saving model (epoch =  179, val loss = 1.7791, train loss = 1.4715)\n",
            "Saving model (epoch =  180, val loss = 1.7777, train loss = 1.6334)\n",
            "Saving model (epoch =  181, val loss = 1.7767, train loss = 1.5660)\n",
            "Saving model (epoch =  182, val loss = 1.7757, train loss = 1.6200)\n",
            "Saving model (epoch =  183, val loss = 1.7746, train loss = 1.5479)\n",
            "Saving model (epoch =  184, val loss = 1.7737, train loss = 1.4298)\n",
            "Saving model (epoch =  185, val loss = 1.7730, train loss = 1.7838)\n",
            "Saving model (epoch =  186, val loss = 1.7716, train loss = 1.7293)\n",
            "Saving model (epoch =  187, val loss = 1.7713, train loss = 1.5741)\n",
            "Saving model (epoch =  188, val loss = 1.7697, train loss = 1.4567)\n",
            "Saving model (epoch =  189, val loss = 1.7694, train loss = 1.4617)\n",
            "Saving model (epoch =  190, val loss = 1.7678, train loss = 1.4530)\n",
            "Saving model (epoch =  191, val loss = 1.7666, train loss = 1.5007)\n",
            "Saving model (epoch =  192, val loss = 1.7654, train loss = 1.4832)\n",
            "Saving model (epoch =  193, val loss = 1.7646, train loss = 1.5250)\n",
            "Saving model (epoch =  194, val loss = 1.7641, train loss = 1.6952)\n",
            "Saving model (epoch =  195, val loss = 1.7629, train loss = 1.5819)\n",
            "Saving model (epoch =  196, val loss = 1.7620, train loss = 1.6469)\n",
            "Saving model (epoch =  197, val loss = 1.7611, train loss = 1.5303)\n",
            "Saving model (epoch =  198, val loss = 1.7601, train loss = 1.5805)\n",
            "Saving model (epoch =  199, val loss = 1.7591, train loss = 1.5851)\n",
            "Saving model (epoch =  200, val loss = 1.7579, train loss = 1.5137)\n",
            "Saving model (epoch =  201, val loss = 1.7573, train loss = 1.7763)\n",
            "Saving model (epoch =  202, val loss = 1.7562, train loss = 1.6801)\n",
            "Saving model (epoch =  203, val loss = 1.7554, train loss = 1.3700)\n",
            "Saving model (epoch =  204, val loss = 1.7547, train loss = 1.5367)\n",
            "Saving model (epoch =  205, val loss = 1.7537, train loss = 1.4169)\n",
            "Saving model (epoch =  206, val loss = 1.7527, train loss = 1.4808)\n",
            "Saving model (epoch =  207, val loss = 1.7520, train loss = 1.4582)\n",
            "Saving model (epoch =  208, val loss = 1.7511, train loss = 1.6285)\n",
            "Saving model (epoch =  209, val loss = 1.7502, train loss = 1.4862)\n",
            "Saving model (epoch =  210, val loss = 1.7494, train loss = 1.7179)\n",
            "Saving model (epoch =  211, val loss = 1.7485, train loss = 1.4273)\n",
            "Saving model (epoch =  212, val loss = 1.7477, train loss = 1.7480)\n",
            "Saving model (epoch =  213, val loss = 1.7465, train loss = 1.6208)\n",
            "Saving model (epoch =  214, val loss = 1.7459, train loss = 1.6646)\n",
            "Saving model (epoch =  215, val loss = 1.7453, train loss = 1.4508)\n",
            "Saving model (epoch =  216, val loss = 1.7437, train loss = 1.5884)\n",
            "Saving model (epoch =  217, val loss = 1.7433, train loss = 1.5617)\n",
            "Saving model (epoch =  218, val loss = 1.7427, train loss = 1.7652)\n",
            "Saving model (epoch =  219, val loss = 1.7417, train loss = 1.6183)\n",
            "Saving model (epoch =  220, val loss = 1.7407, train loss = 1.4854)\n",
            "Saving model (epoch =  221, val loss = 1.7403, train loss = 1.7197)\n",
            "Saving model (epoch =  222, val loss = 1.7392, train loss = 1.6937)\n",
            "Saving model (epoch =  223, val loss = 1.7384, train loss = 1.5831)\n",
            "Saving model (epoch =  224, val loss = 1.7376, train loss = 1.5080)\n",
            "Saving model (epoch =  225, val loss = 1.7362, train loss = 1.5096)\n",
            "Saving model (epoch =  226, val loss = 1.7358, train loss = 1.4868)\n",
            "Saving model (epoch =  227, val loss = 1.7350, train loss = 1.5317)\n",
            "Saving model (epoch =  228, val loss = 1.7344, train loss = 1.8559)\n",
            "Saving model (epoch =  229, val loss = 1.7335, train loss = 1.3857)\n",
            "Saving model (epoch =  230, val loss = 1.7329, train loss = 1.3543)\n",
            "Saving model (epoch =  231, val loss = 1.7320, train loss = 1.5631)\n",
            "Saving model (epoch =  232, val loss = 1.7311, train loss = 1.6482)\n",
            "Saving model (epoch =  233, val loss = 1.7303, train loss = 1.4514)\n",
            "Saving model (epoch =  234, val loss = 1.7298, train loss = 1.4157)\n",
            "Saving model (epoch =  235, val loss = 1.7291, train loss = 1.6027)\n",
            "Saving model (epoch =  236, val loss = 1.7280, train loss = 1.4690)\n",
            "Saving model (epoch =  237, val loss = 1.7271, train loss = 1.4807)\n",
            "Saving model (epoch =  238, val loss = 1.7266, train loss = 1.4674)\n",
            "Saving model (epoch =  239, val loss = 1.7259, train loss = 1.5454)\n",
            "Saving model (epoch =  240, val loss = 1.7251, train loss = 1.3972)\n",
            "Saving model (epoch =  241, val loss = 1.7245, train loss = 1.4998)\n",
            "Saving model (epoch =  242, val loss = 1.7237, train loss = 1.4990)\n",
            "Saving model (epoch =  243, val loss = 1.7230, train loss = 1.3445)\n",
            "Saving model (epoch =  244, val loss = 1.7222, train loss = 1.4201)\n",
            "Saving model (epoch =  245, val loss = 1.7219, train loss = 1.5304)\n",
            "Saving model (epoch =  246, val loss = 1.7210, train loss = 1.4999)\n",
            "Saving model (epoch =  247, val loss = 1.7201, train loss = 1.5200)\n",
            "Saving model (epoch =  248, val loss = 1.7193, train loss = 1.5551)\n",
            "Saving model (epoch =  250, val loss = 1.7182, train loss = 1.4635)\n",
            "Saving model (epoch =  251, val loss = 1.7174, train loss = 1.4358)\n",
            "Saving model (epoch =  252, val loss = 1.7165, train loss = 1.5725)\n",
            "Saving model (epoch =  253, val loss = 1.7161, train loss = 1.3920)\n",
            "Saving model (epoch =  254, val loss = 1.7158, train loss = 1.3603)\n",
            "Saving model (epoch =  255, val loss = 1.7147, train loss = 1.6960)\n",
            "Saving model (epoch =  256, val loss = 1.7142, train loss = 1.6225)\n",
            "Saving model (epoch =  257, val loss = 1.7136, train loss = 1.5461)\n",
            "Saving model (epoch =  258, val loss = 1.7130, train loss = 1.5133)\n",
            "Saving model (epoch =  259, val loss = 1.7120, train loss = 1.4402)\n",
            "Saving model (epoch =  260, val loss = 1.7113, train loss = 1.5534)\n",
            "Saving model (epoch =  261, val loss = 1.7105, train loss = 1.6621)\n",
            "Saving model (epoch =  262, val loss = 1.7104, train loss = 1.3787)\n",
            "Saving model (epoch =  263, val loss = 1.7095, train loss = 1.5141)\n",
            "Saving model (epoch =  264, val loss = 1.7086, train loss = 1.4805)\n",
            "Saving model (epoch =  266, val loss = 1.7073, train loss = 1.6245)\n",
            "Saving model (epoch =  267, val loss = 1.7071, train loss = 1.7530)\n",
            "Saving model (epoch =  268, val loss = 1.7063, train loss = 1.4990)\n",
            "Saving model (epoch =  269, val loss = 1.7062, train loss = 1.5367)\n",
            "Saving model (epoch =  270, val loss = 1.7052, train loss = 1.5990)\n",
            "Saving model (epoch =  271, val loss = 1.7043, train loss = 1.4435)\n",
            "Saving model (epoch =  272, val loss = 1.7034, train loss = 1.4374)\n",
            "Saving model (epoch =  273, val loss = 1.7032, train loss = 1.3581)\n",
            "Saving model (epoch =  274, val loss = 1.7026, train loss = 1.5135)\n",
            "Saving model (epoch =  276, val loss = 1.7012, train loss = 1.4722)\n",
            "Saving model (epoch =  277, val loss = 1.7006, train loss = 1.5052)\n",
            "Saving model (epoch =  278, val loss = 1.7000, train loss = 1.4423)\n",
            "Saving model (epoch =  279, val loss = 1.6998, train loss = 1.4415)\n",
            "Saving model (epoch =  280, val loss = 1.6993, train loss = 1.5748)\n",
            "Saving model (epoch =  281, val loss = 1.6982, train loss = 1.5320)\n",
            "Saving model (epoch =  282, val loss = 1.6978, train loss = 1.5911)\n",
            "Saving model (epoch =  283, val loss = 1.6971, train loss = 1.5539)\n",
            "Saving model (epoch =  284, val loss = 1.6966, train loss = 1.4946)\n",
            "Saving model (epoch =  285, val loss = 1.6962, train loss = 1.5100)\n",
            "Saving model (epoch =  286, val loss = 1.6961, train loss = 1.5235)\n",
            "Saving model (epoch =  287, val loss = 1.6949, train loss = 1.4869)\n",
            "Saving model (epoch =  288, val loss = 1.6944, train loss = 1.4699)\n",
            "Saving model (epoch =  289, val loss = 1.6932, train loss = 1.5333)\n",
            "Saving model (epoch =  290, val loss = 1.6926, train loss = 1.6031)\n",
            "Saving model (epoch =  291, val loss = 1.6923, train loss = 1.3835)\n",
            "Saving model (epoch =  292, val loss = 1.6918, train loss = 1.3619)\n",
            "Saving model (epoch =  293, val loss = 1.6913, train loss = 1.4096)\n",
            "Saving model (epoch =  294, val loss = 1.6909, train loss = 1.6031)\n",
            "Saving model (epoch =  295, val loss = 1.6901, train loss = 1.6303)\n",
            "Saving model (epoch =  296, val loss = 1.6894, train loss = 1.5908)\n",
            "Saving model (epoch =  297, val loss = 1.6888, train loss = 1.4286)\n",
            "Saving model (epoch =  298, val loss = 1.6884, train loss = 1.4565)\n",
            "Saving model (epoch =  299, val loss = 1.6880, train loss = 1.5774)\n",
            "Saving model (epoch =  300, val loss = 1.6873, train loss = 1.4741)\n",
            "Saving model (epoch =  301, val loss = 1.6865, train loss = 1.2545)\n",
            "Saving model (epoch =  302, val loss = 1.6858, train loss = 1.4748)\n",
            "Saving model (epoch =  303, val loss = 1.6857, train loss = 1.5136)\n",
            "Saving model (epoch =  304, val loss = 1.6856, train loss = 1.5239)\n",
            "Saving model (epoch =  305, val loss = 1.6848, train loss = 1.5890)\n",
            "Saving model (epoch =  306, val loss = 1.6837, train loss = 1.4811)\n",
            "Saving model (epoch =  307, val loss = 1.6832, train loss = 1.4123)\n",
            "Saving model (epoch =  308, val loss = 1.6831, train loss = 1.5846)\n",
            "Saving model (epoch =  309, val loss = 1.6823, train loss = 1.5621)\n",
            "Saving model (epoch =  310, val loss = 1.6820, train loss = 1.6334)\n",
            "Saving model (epoch =  311, val loss = 1.6811, train loss = 1.3659)\n",
            "Saving model (epoch =  312, val loss = 1.6808, train loss = 1.5068)\n",
            "Saving model (epoch =  313, val loss = 1.6803, train loss = 1.4433)\n",
            "Saving model (epoch =  314, val loss = 1.6797, train loss = 1.4878)\n",
            "Saving model (epoch =  315, val loss = 1.6796, train loss = 1.6091)\n",
            "Saving model (epoch =  316, val loss = 1.6791, train loss = 1.5155)\n",
            "Saving model (epoch =  317, val loss = 1.6782, train loss = 1.5606)\n",
            "Saving model (epoch =  318, val loss = 1.6775, train loss = 1.2587)\n",
            "Saving model (epoch =  320, val loss = 1.6763, train loss = 1.5383)\n",
            "Saving model (epoch =  322, val loss = 1.6755, train loss = 1.4002)\n",
            "Saving model (epoch =  324, val loss = 1.6746, train loss = 1.5661)\n",
            "Saving model (epoch =  325, val loss = 1.6741, train loss = 1.5721)\n",
            "Saving model (epoch =  326, val loss = 1.6738, train loss = 1.4396)\n",
            "Saving model (epoch =  328, val loss = 1.6728, train loss = 1.5190)\n",
            "Saving model (epoch =  329, val loss = 1.6724, train loss = 1.3812)\n",
            "Saving model (epoch =  330, val loss = 1.6720, train loss = 1.5592)\n",
            "Saving model (epoch =  331, val loss = 1.6713, train loss = 1.6455)\n",
            "Saving model (epoch =  332, val loss = 1.6709, train loss = 1.5051)\n",
            "Saving model (epoch =  333, val loss = 1.6699, train loss = 1.3320)\n",
            "Saving model (epoch =  336, val loss = 1.6688, train loss = 1.4572)\n",
            "Saving model (epoch =  337, val loss = 1.6685, train loss = 1.3670)\n",
            "Saving model (epoch =  338, val loss = 1.6679, train loss = 1.5589)\n",
            "Saving model (epoch =  339, val loss = 1.6678, train loss = 1.4208)\n",
            "Saving model (epoch =  340, val loss = 1.6674, train loss = 1.5444)\n",
            "Saving model (epoch =  341, val loss = 1.6667, train loss = 1.4964)\n",
            "Saving model (epoch =  342, val loss = 1.6663, train loss = 1.6113)\n",
            "Saving model (epoch =  343, val loss = 1.6659, train loss = 1.5361)\n",
            "Saving model (epoch =  344, val loss = 1.6653, train loss = 1.4521)\n",
            "Saving model (epoch =  345, val loss = 1.6648, train loss = 1.3630)\n",
            "Saving model (epoch =  346, val loss = 1.6643, train loss = 1.5108)\n",
            "Saving model (epoch =  347, val loss = 1.6641, train loss = 1.4669)\n",
            "Saving model (epoch =  349, val loss = 1.6636, train loss = 1.4063)\n",
            "Saving model (epoch =  350, val loss = 1.6635, train loss = 1.5388)\n",
            "Saving model (epoch =  351, val loss = 1.6622, train loss = 1.3584)\n",
            "Saving model (epoch =  352, val loss = 1.6616, train loss = 1.6554)\n",
            "Saving model (epoch =  354, val loss = 1.6612, train loss = 1.5391)\n",
            "Saving model (epoch =  355, val loss = 1.6606, train loss = 1.4600)\n",
            "Saving model (epoch =  356, val loss = 1.6604, train loss = 1.3407)\n",
            "Saving model (epoch =  357, val loss = 1.6603, train loss = 1.4272)\n",
            "Saving model (epoch =  358, val loss = 1.6596, train loss = 1.4658)\n",
            "Saving model (epoch =  359, val loss = 1.6593, train loss = 1.6265)\n",
            "Saving model (epoch =  360, val loss = 1.6590, train loss = 1.5458)\n",
            "Saving model (epoch =  361, val loss = 1.6581, train loss = 1.3588)\n",
            "Saving model (epoch =  362, val loss = 1.6579, train loss = 1.6037)\n",
            "Saving model (epoch =  363, val loss = 1.6573, train loss = 1.3951)\n",
            "Saving model (epoch =  365, val loss = 1.6569, train loss = 1.4531)\n",
            "Saving model (epoch =  366, val loss = 1.6562, train loss = 1.5247)\n",
            "Saving model (epoch =  367, val loss = 1.6559, train loss = 1.3943)\n",
            "Saving model (epoch =  368, val loss = 1.6558, train loss = 1.4027)\n",
            "Saving model (epoch =  369, val loss = 1.6555, train loss = 1.5832)\n",
            "Saving model (epoch =  370, val loss = 1.6548, train loss = 1.4418)\n",
            "Saving model (epoch =  371, val loss = 1.6545, train loss = 1.4109)\n",
            "Saving model (epoch =  372, val loss = 1.6544, train loss = 1.3206)\n",
            "Saving model (epoch =  373, val loss = 1.6537, train loss = 1.5016)\n",
            "Saving model (epoch =  374, val loss = 1.6532, train loss = 1.5276)\n",
            "Saving model (epoch =  375, val loss = 1.6528, train loss = 1.6429)\n",
            "Saving model (epoch =  376, val loss = 1.6525, train loss = 1.5446)\n",
            "Saving model (epoch =  377, val loss = 1.6520, train loss = 1.4688)\n",
            "Saving model (epoch =  379, val loss = 1.6517, train loss = 1.5071)\n",
            "Saving model (epoch =  380, val loss = 1.6516, train loss = 1.3655)\n",
            "Saving model (epoch =  381, val loss = 1.6511, train loss = 1.5582)\n",
            "Saving model (epoch =  382, val loss = 1.6505, train loss = 1.5434)\n",
            "Saving model (epoch =  383, val loss = 1.6502, train loss = 1.5155)\n",
            "Saving model (epoch =  384, val loss = 1.6498, train loss = 1.5625)\n",
            "Saving model (epoch =  385, val loss = 1.6497, train loss = 1.4917)\n",
            "Saving model (epoch =  386, val loss = 1.6492, train loss = 1.5206)\n",
            "Saving model (epoch =  387, val loss = 1.6486, train loss = 1.4482)\n",
            "Saving model (epoch =  388, val loss = 1.6485, train loss = 1.5268)\n",
            "Saving model (epoch =  389, val loss = 1.6479, train loss = 1.4325)\n",
            "Saving model (epoch =  390, val loss = 1.6478, train loss = 1.4227)\n",
            "Saving model (epoch =  391, val loss = 1.6477, train loss = 1.3612)\n",
            "Saving model (epoch =  392, val loss = 1.6468, train loss = 1.3534)\n",
            "Saving model (epoch =  393, val loss = 1.6465, train loss = 1.3759)\n",
            "Saving model (epoch =  394, val loss = 1.6464, train loss = 1.5813)\n",
            "Saving model (epoch =  395, val loss = 1.6458, train loss = 1.5890)\n",
            "Saving model (epoch =  398, val loss = 1.6452, train loss = 1.4919)\n",
            "Saving model (epoch =  399, val loss = 1.6445, train loss = 1.5715)\n",
            "Saving model (epoch =  400, val loss = 1.6442, train loss = 1.6171)\n",
            "Saving model (epoch =  401, val loss = 1.6441, train loss = 1.4360)\n",
            "Saving model (epoch =  402, val loss = 1.6436, train loss = 1.2416)\n",
            "Saving model (epoch =  404, val loss = 1.6426, train loss = 1.5069)\n",
            "Saving model (epoch =  406, val loss = 1.6422, train loss = 1.4751)\n",
            "Saving model (epoch =  408, val loss = 1.6417, train loss = 1.3337)\n",
            "Saving model (epoch =  409, val loss = 1.6414, train loss = 1.3385)\n",
            "Saving model (epoch =  411, val loss = 1.6406, train loss = 1.6493)\n",
            "Saving model (epoch =  412, val loss = 1.6405, train loss = 1.5506)\n",
            "Saving model (epoch =  414, val loss = 1.6404, train loss = 1.5141)\n",
            "Saving model (epoch =  415, val loss = 1.6401, train loss = 1.4917)\n",
            "Saving model (epoch =  416, val loss = 1.6394, train loss = 1.5150)\n",
            "Saving model (epoch =  418, val loss = 1.6382, train loss = 1.3730)\n",
            "Saving model (epoch =  421, val loss = 1.6377, train loss = 1.4581)\n",
            "Saving model (epoch =  422, val loss = 1.6373, train loss = 1.4599)\n",
            "Saving model (epoch =  424, val loss = 1.6368, train loss = 1.4770)\n",
            "Saving model (epoch =  426, val loss = 1.6364, train loss = 1.4311)\n",
            "Saving model (epoch =  427, val loss = 1.6359, train loss = 1.4534)\n",
            "Saving model (epoch =  428, val loss = 1.6359, train loss = 1.2920)\n",
            "Saving model (epoch =  429, val loss = 1.6354, train loss = 1.4398)\n",
            "Saving model (epoch =  431, val loss = 1.6348, train loss = 1.4451)\n",
            "Saving model (epoch =  432, val loss = 1.6345, train loss = 1.4070)\n",
            "Saving model (epoch =  433, val loss = 1.6341, train loss = 1.4517)\n",
            "Saving model (epoch =  434, val loss = 1.6340, train loss = 1.3750)\n",
            "Saving model (epoch =  435, val loss = 1.6340, train loss = 1.3322)\n",
            "Saving model (epoch =  436, val loss = 1.6337, train loss = 1.5848)\n",
            "Saving model (epoch =  437, val loss = 1.6334, train loss = 1.5210)\n",
            "Saving model (epoch =  438, val loss = 1.6330, train loss = 1.2476)\n",
            "Saving model (epoch =  440, val loss = 1.6322, train loss = 1.4375)\n",
            "Saving model (epoch =  441, val loss = 1.6319, train loss = 1.5044)\n",
            "Saving model (epoch =  443, val loss = 1.6317, train loss = 1.4790)\n",
            "Saving model (epoch =  444, val loss = 1.6311, train loss = 1.3515)\n",
            "Saving model (epoch =  446, val loss = 1.6309, train loss = 1.6032)\n",
            "Saving model (epoch =  447, val loss = 1.6309, train loss = 1.3198)\n",
            "Saving model (epoch =  449, val loss = 1.6301, train loss = 1.5532)\n",
            "Saving model (epoch =  450, val loss = 1.6296, train loss = 1.3888)\n",
            "Saving model (epoch =  451, val loss = 1.6292, train loss = 1.4124)\n",
            "Saving model (epoch =  452, val loss = 1.6291, train loss = 1.2618)\n",
            "Saving model (epoch =  453, val loss = 1.6289, train loss = 1.5929)\n",
            "Saving model (epoch =  454, val loss = 1.6289, train loss = 1.4719)\n",
            "Saving model (epoch =  455, val loss = 1.6285, train loss = 1.3810)\n",
            "Saving model (epoch =  456, val loss = 1.6280, train loss = 1.4914)\n",
            "Saving model (epoch =  457, val loss = 1.6277, train loss = 1.5651)\n",
            "Saving model (epoch =  458, val loss = 1.6274, train loss = 1.4895)\n",
            "Saving model (epoch =  460, val loss = 1.6272, train loss = 1.4982)\n",
            "Saving model (epoch =  461, val loss = 1.6269, train loss = 1.4249)\n",
            "Saving model (epoch =  462, val loss = 1.6269, train loss = 1.3958)\n",
            "Saving model (epoch =  463, val loss = 1.6260, train loss = 1.5833)\n",
            "Saving model (epoch =  464, val loss = 1.6259, train loss = 1.4608)\n",
            "Saving model (epoch =  467, val loss = 1.6258, train loss = 1.4001)\n",
            "Saving model (epoch =  468, val loss = 1.6253, train loss = 1.4018)\n",
            "Saving model (epoch =  469, val loss = 1.6253, train loss = 1.3011)\n",
            "Saving model (epoch =  470, val loss = 1.6246, train loss = 1.4093)\n",
            "Saving model (epoch =  472, val loss = 1.6242, train loss = 1.5046)\n",
            "Saving model (epoch =  473, val loss = 1.6237, train loss = 1.4417)\n",
            "Saving model (epoch =  474, val loss = 1.6236, train loss = 1.2421)\n",
            "Saving model (epoch =  475, val loss = 1.6228, train loss = 1.4599)\n",
            "Saving model (epoch =  480, val loss = 1.6220, train loss = 1.3882)\n",
            "Saving model (epoch =  481, val loss = 1.6215, train loss = 1.6016)\n",
            "Saving model (epoch =  483, val loss = 1.6215, train loss = 1.3175)\n",
            "Saving model (epoch =  485, val loss = 1.6212, train loss = 1.6852)\n",
            "Saving model (epoch =  486, val loss = 1.6209, train loss = 1.4638)\n",
            "Saving model (epoch =  488, val loss = 1.6205, train loss = 1.3204)\n",
            "Saving model (epoch =  489, val loss = 1.6200, train loss = 1.4240)\n",
            "Saving model (epoch =  492, val loss = 1.6193, train loss = 1.5004)\n",
            "Saving model (epoch =  493, val loss = 1.6191, train loss = 1.2943)\n",
            "Saving model (epoch =  495, val loss = 1.6188, train loss = 1.3212)\n",
            "Saving model (epoch =  496, val loss = 1.6186, train loss = 1.3097)\n",
            "Saving model (epoch =  497, val loss = 1.6184, train loss = 1.5157)\n",
            "Saving model (epoch =  500, val loss = 1.6179, train loss = 1.5911)\n",
            "Saving model (epoch =  501, val loss = 1.6177, train loss = 1.3955)\n",
            "Saving model (epoch =  503, val loss = 1.6170, train loss = 1.4566)\n",
            "Saving model (epoch =  504, val loss = 1.6170, train loss = 1.5151)\n",
            "Saving model (epoch =  505, val loss = 1.6168, train loss = 1.3997)\n",
            "Saving model (epoch =  507, val loss = 1.6160, train loss = 1.4709)\n",
            "Saving model (epoch =  508, val loss = 1.6160, train loss = 1.3420)\n",
            "Saving model (epoch =  510, val loss = 1.6156, train loss = 1.1885)\n",
            "Saving model (epoch =  511, val loss = 1.6151, train loss = 1.3403)\n",
            "Saving model (epoch =  512, val loss = 1.6150, train loss = 1.4326)\n",
            "Saving model (epoch =  513, val loss = 1.6147, train loss = 1.5738)\n",
            "Saving model (epoch =  516, val loss = 1.6146, train loss = 1.4487)\n",
            "Saving model (epoch =  518, val loss = 1.6143, train loss = 1.4911)\n",
            "Saving model (epoch =  520, val loss = 1.6134, train loss = 1.2760)\n",
            "Saving model (epoch =  521, val loss = 1.6133, train loss = 1.4997)\n",
            "Saving model (epoch =  522, val loss = 1.6133, train loss = 1.4584)\n",
            "Saving model (epoch =  523, val loss = 1.6131, train loss = 1.3320)\n",
            "Saving model (epoch =  524, val loss = 1.6128, train loss = 1.3550)\n",
            "Saving model (epoch =  526, val loss = 1.6124, train loss = 1.5856)\n",
            "Saving model (epoch =  528, val loss = 1.6120, train loss = 1.4620)\n",
            "Saving model (epoch =  529, val loss = 1.6115, train loss = 1.3580)\n",
            "Saving model (epoch =  530, val loss = 1.6115, train loss = 1.3194)\n",
            "Saving model (epoch =  532, val loss = 1.6110, train loss = 1.4081)\n",
            "Saving model (epoch =  533, val loss = 1.6108, train loss = 1.2700)\n",
            "Saving model (epoch =  535, val loss = 1.6104, train loss = 1.2990)\n",
            "Saving model (epoch =  536, val loss = 1.6101, train loss = 1.4279)\n",
            "Saving model (epoch =  541, val loss = 1.6096, train loss = 1.4016)\n",
            "Saving model (epoch =  542, val loss = 1.6092, train loss = 1.5018)\n",
            "Saving model (epoch =  543, val loss = 1.6090, train loss = 1.2432)\n",
            "Saving model (epoch =  545, val loss = 1.6089, train loss = 1.3534)\n",
            "Saving model (epoch =  546, val loss = 1.6088, train loss = 1.4641)\n",
            "Saving model (epoch =  547, val loss = 1.6082, train loss = 1.5222)\n",
            "Saving model (epoch =  548, val loss = 1.6080, train loss = 1.4917)\n",
            "Saving model (epoch =  551, val loss = 1.6078, train loss = 1.4703)\n",
            "Saving model (epoch =  552, val loss = 1.6076, train loss = 1.3301)\n",
            "Saving model (epoch =  553, val loss = 1.6076, train loss = 1.4504)\n",
            "Saving model (epoch =  554, val loss = 1.6068, train loss = 1.3589)\n",
            "Saving model (epoch =  555, val loss = 1.6066, train loss = 1.4019)\n",
            "Saving model (epoch =  556, val loss = 1.6065, train loss = 1.5127)\n",
            "Saving model (epoch =  558, val loss = 1.6065, train loss = 1.7628)\n",
            "Saving model (epoch =  559, val loss = 1.6063, train loss = 1.4041)\n",
            "Saving model (epoch =  560, val loss = 1.6061, train loss = 1.3666)\n",
            "Saving model (epoch =  563, val loss = 1.6055, train loss = 1.3699)\n",
            "Saving model (epoch =  564, val loss = 1.6050, train loss = 1.4669)\n",
            "Saving model (epoch =  565, val loss = 1.6050, train loss = 1.6442)\n",
            "Saving model (epoch =  567, val loss = 1.6046, train loss = 1.5028)\n",
            "Saving model (epoch =  568, val loss = 1.6043, train loss = 1.5174)\n",
            "Saving model (epoch =  572, val loss = 1.6038, train loss = 1.3802)\n",
            "Saving model (epoch =  574, val loss = 1.6038, train loss = 1.4364)\n",
            "Saving model (epoch =  577, val loss = 1.6033, train loss = 1.4182)\n",
            "Saving model (epoch =  578, val loss = 1.6031, train loss = 1.4572)\n",
            "Saving model (epoch =  580, val loss = 1.6023, train loss = 1.2661)\n",
            "Saving model (epoch =  584, val loss = 1.6016, train loss = 1.3272)\n",
            "Saving model (epoch =  585, val loss = 1.6016, train loss = 1.3010)\n",
            "Saving model (epoch =  586, val loss = 1.6011, train loss = 1.3109)\n",
            "Saving model (epoch =  588, val loss = 1.6007, train loss = 1.5890)\n",
            "Saving model (epoch =  593, val loss = 1.6007, train loss = 1.4095)\n",
            "Saving model (epoch =  594, val loss = 1.6004, train loss = 1.3301)\n",
            "Saving model (epoch =  595, val loss = 1.6000, train loss = 1.3255)\n",
            "Saving model (epoch =  597, val loss = 1.5999, train loss = 1.3648)\n",
            "Saving model (epoch =  599, val loss = 1.5995, train loss = 1.2337)\n",
            "Saving model (epoch =  602, val loss = 1.5993, train loss = 1.2143)\n",
            "Saving model (epoch =  603, val loss = 1.5988, train loss = 1.3961)\n",
            "Saving model (epoch =  604, val loss = 1.5987, train loss = 1.3796)\n",
            "Saving model (epoch =  605, val loss = 1.5987, train loss = 1.3870)\n",
            "Saving model (epoch =  608, val loss = 1.5984, train loss = 1.4037)\n",
            "Saving model (epoch =  609, val loss = 1.5981, train loss = 1.4471)\n",
            "Saving model (epoch =  611, val loss = 1.5980, train loss = 1.3317)\n",
            "Saving model (epoch =  612, val loss = 1.5976, train loss = 1.3720)\n",
            "Saving model (epoch =  613, val loss = 1.5975, train loss = 1.3659)\n",
            "Saving model (epoch =  614, val loss = 1.5975, train loss = 1.3451)\n",
            "Saving model (epoch =  615, val loss = 1.5973, train loss = 1.4198)\n",
            "Saving model (epoch =  618, val loss = 1.5967, train loss = 1.2990)\n",
            "Saving model (epoch =  619, val loss = 1.5964, train loss = 1.4694)\n",
            "Saving model (epoch =  622, val loss = 1.5958, train loss = 1.5752)\n",
            "Saving model (epoch =  627, val loss = 1.5955, train loss = 1.4225)\n",
            "Saving model (epoch =  628, val loss = 1.5953, train loss = 1.4136)\n",
            "Saving model (epoch =  629, val loss = 1.5949, train loss = 1.4999)\n",
            "Saving model (epoch =  630, val loss = 1.5949, train loss = 1.5179)\n",
            "Saving model (epoch =  632, val loss = 1.5944, train loss = 1.4915)\n",
            "Saving model (epoch =  633, val loss = 1.5940, train loss = 1.4236)\n",
            "Saving model (epoch =  635, val loss = 1.5938, train loss = 1.3035)\n",
            "Saving model (epoch =  636, val loss = 1.5938, train loss = 1.4973)\n",
            "Saving model (epoch =  637, val loss = 1.5935, train loss = 1.5039)\n",
            "Saving model (epoch =  640, val loss = 1.5932, train loss = 1.3645)\n",
            "Saving model (epoch =  643, val loss = 1.5932, train loss = 1.2708)\n",
            "Saving model (epoch =  644, val loss = 1.5928, train loss = 1.3063)\n",
            "Saving model (epoch =  645, val loss = 1.5927, train loss = 1.2977)\n",
            "Saving model (epoch =  648, val loss = 1.5920, train loss = 1.4162)\n",
            "Saving model (epoch =  652, val loss = 1.5917, train loss = 1.4247)\n",
            "Saving model (epoch =  653, val loss = 1.5909, train loss = 1.2808)\n",
            "Saving model (epoch =  660, val loss = 1.5905, train loss = 1.2772)\n",
            "Saving model (epoch =  661, val loss = 1.5904, train loss = 1.4017)\n",
            "Saving model (epoch =  663, val loss = 1.5901, train loss = 1.4817)\n",
            "Saving model (epoch =  664, val loss = 1.5900, train loss = 1.3482)\n",
            "Saving model (epoch =  666, val loss = 1.5895, train loss = 1.5312)\n",
            "Saving model (epoch =  669, val loss = 1.5892, train loss = 1.2468)\n",
            "Saving model (epoch =  671, val loss = 1.5891, train loss = 1.4051)\n",
            "Saving model (epoch =  673, val loss = 1.5888, train loss = 1.2720)\n",
            "Saving model (epoch =  674, val loss = 1.5887, train loss = 1.5465)\n",
            "Saving model (epoch =  677, val loss = 1.5886, train loss = 1.3739)\n",
            "Saving model (epoch =  679, val loss = 1.5884, train loss = 1.5071)\n",
            "Saving model (epoch =  680, val loss = 1.5882, train loss = 1.3849)\n",
            "Saving model (epoch =  681, val loss = 1.5872, train loss = 1.4614)\n",
            "Saving model (epoch =  688, val loss = 1.5870, train loss = 1.3241)\n",
            "Saving model (epoch =  689, val loss = 1.5866, train loss = 1.4510)\n",
            "Saving model (epoch =  691, val loss = 1.5866, train loss = 1.3316)\n",
            "Saving model (epoch =  692, val loss = 1.5865, train loss = 1.3739)\n",
            "Saving model (epoch =  694, val loss = 1.5860, train loss = 1.2928)\n",
            "Saving model (epoch =  698, val loss = 1.5854, train loss = 1.3894)\n",
            "Saving model (epoch =  701, val loss = 1.5852, train loss = 1.1772)\n",
            "Saving model (epoch =  702, val loss = 1.5851, train loss = 1.4777)\n",
            "Saving model (epoch =  705, val loss = 1.5851, train loss = 1.6046)\n",
            "Saving model (epoch =  706, val loss = 1.5850, train loss = 1.3243)\n",
            "Saving model (epoch =  707, val loss = 1.5848, train loss = 1.3633)\n",
            "Saving model (epoch =  708, val loss = 1.5845, train loss = 1.4509)\n",
            "Saving model (epoch =  709, val loss = 1.5842, train loss = 1.3635)\n",
            "Saving model (epoch =  711, val loss = 1.5841, train loss = 1.2904)\n",
            "Saving model (epoch =  712, val loss = 1.5839, train loss = 1.3359)\n",
            "Saving model (epoch =  714, val loss = 1.5838, train loss = 1.4184)\n",
            "Saving model (epoch =  716, val loss = 1.5838, train loss = 1.4595)\n",
            "Saving model (epoch =  717, val loss = 1.5833, train loss = 1.2634)\n",
            "Saving model (epoch =  719, val loss = 1.5833, train loss = 1.4324)\n",
            "Saving model (epoch =  723, val loss = 1.5827, train loss = 1.4511)\n",
            "Saving model (epoch =  724, val loss = 1.5827, train loss = 1.3702)\n",
            "Saving model (epoch =  727, val loss = 1.5819, train loss = 1.4091)\n",
            "Saving model (epoch =  733, val loss = 1.5815, train loss = 1.5267)\n",
            "Saving model (epoch =  736, val loss = 1.5813, train loss = 1.4760)\n",
            "Saving model (epoch =  739, val loss = 1.5813, train loss = 1.4462)\n",
            "Saving model (epoch =  740, val loss = 1.5813, train loss = 1.3800)\n",
            "Saving model (epoch =  741, val loss = 1.5804, train loss = 1.3836)\n",
            "Saving model (epoch =  748, val loss = 1.5801, train loss = 1.5111)\n",
            "Saving model (epoch =  752, val loss = 1.5798, train loss = 1.4558)\n",
            "Saving model (epoch =  754, val loss = 1.5796, train loss = 1.5204)\n",
            "Saving model (epoch =  756, val loss = 1.5795, train loss = 1.3771)\n",
            "Saving model (epoch =  757, val loss = 1.5792, train loss = 1.2934)\n",
            "Saving model (epoch =  758, val loss = 1.5792, train loss = 1.5863)\n",
            "Saving model (epoch =  760, val loss = 1.5788, train loss = 1.3474)\n",
            "Saving model (epoch =  764, val loss = 1.5783, train loss = 1.3895)\n",
            "Saving model (epoch =  767, val loss = 1.5780, train loss = 1.3160)\n",
            "Saving model (epoch =  769, val loss = 1.5779, train loss = 1.4543)\n",
            "Saving model (epoch =  770, val loss = 1.5776, train loss = 1.5279)\n",
            "Saving model (epoch =  775, val loss = 1.5775, train loss = 1.4345)\n",
            "Saving model (epoch =  777, val loss = 1.5773, train loss = 1.2735)\n",
            "Saving model (epoch =  783, val loss = 1.5766, train loss = 1.3132)\n",
            "Saving model (epoch =  786, val loss = 1.5763, train loss = 1.4587)\n",
            "Saving model (epoch =  787, val loss = 1.5762, train loss = 1.2850)\n",
            "Saving model (epoch =  790, val loss = 1.5761, train loss = 1.3807)\n",
            "Saving model (epoch =  791, val loss = 1.5760, train loss = 1.4045)\n",
            "Saving model (epoch =  792, val loss = 1.5756, train loss = 1.3181)\n",
            "Saving model (epoch =  795, val loss = 1.5755, train loss = 1.3135)\n",
            "Saving model (epoch =  799, val loss = 1.5751, train loss = 1.2482)\n",
            "Saving model (epoch =  802, val loss = 1.5749, train loss = 1.3727)\n",
            "Saving model (epoch =  805, val loss = 1.5747, train loss = 1.2942)\n",
            "Saving model (epoch =  806, val loss = 1.5747, train loss = 1.4115)\n",
            "Saving model (epoch =  808, val loss = 1.5745, train loss = 1.3373)\n",
            "Saving model (epoch =  809, val loss = 1.5744, train loss = 1.3871)\n",
            "Saving model (epoch =  812, val loss = 1.5740, train loss = 1.4291)\n",
            "Saving model (epoch =  814, val loss = 1.5740, train loss = 1.3584)\n",
            "Saving model (epoch =  815, val loss = 1.5740, train loss = 1.3996)\n",
            "Saving model (epoch =  816, val loss = 1.5735, train loss = 1.3382)\n",
            "Saving model (epoch =  819, val loss = 1.5735, train loss = 1.4453)\n",
            "Saving model (epoch =  820, val loss = 1.5732, train loss = 1.3003)\n",
            "Saving model (epoch =  824, val loss = 1.5730, train loss = 1.3640)\n",
            "Saving model (epoch =  826, val loss = 1.5729, train loss = 1.5325)\n",
            "Saving model (epoch =  828, val loss = 1.5726, train loss = 1.4830)\n",
            "Saving model (epoch =  829, val loss = 1.5724, train loss = 1.2844)\n",
            "Saving model (epoch =  834, val loss = 1.5722, train loss = 1.3076)\n",
            "Saving model (epoch =  836, val loss = 1.5722, train loss = 1.5206)\n",
            "Saving model (epoch =  837, val loss = 1.5718, train loss = 1.3765)\n",
            "Saving model (epoch =  840, val loss = 1.5717, train loss = 1.4479)\n",
            "Saving model (epoch =  845, val loss = 1.5713, train loss = 1.5498)\n",
            "Saving model (epoch =  848, val loss = 1.5710, train loss = 1.2824)\n",
            "Saving model (epoch =  849, val loss = 1.5707, train loss = 1.2843)\n",
            "Saving model (epoch =  852, val loss = 1.5701, train loss = 1.3351)\n",
            "Saving model (epoch =  857, val loss = 1.5701, train loss = 1.2630)\n",
            "Saving model (epoch =  861, val loss = 1.5699, train loss = 1.3846)\n",
            "Saving model (epoch =  863, val loss = 1.5697, train loss = 1.2728)\n",
            "Saving model (epoch =  866, val loss = 1.5694, train loss = 1.5475)\n",
            "Saving model (epoch =  873, val loss = 1.5690, train loss = 1.3903)\n",
            "Saving model (epoch =  876, val loss = 1.5690, train loss = 1.3800)\n",
            "Saving model (epoch =  879, val loss = 1.5681, train loss = 1.5037)\n",
            "Saving model (epoch =  889, val loss = 1.5675, train loss = 1.3632)\n",
            "Saving model (epoch =  894, val loss = 1.5669, train loss = 1.4087)\n",
            "Saving model (epoch =  901, val loss = 1.5668, train loss = 1.3257)\n",
            "Saving model (epoch =  907, val loss = 1.5663, train loss = 1.2183)\n",
            "Saving model (epoch =  909, val loss = 1.5660, train loss = 1.4883)\n",
            "Saving model (epoch =  910, val loss = 1.5658, train loss = 1.3184)\n",
            "Saving model (epoch =  913, val loss = 1.5657, train loss = 1.4528)\n",
            "Saving model (epoch =  917, val loss = 1.5654, train loss = 1.5445)\n",
            "Saving model (epoch =  920, val loss = 1.5651, train loss = 1.4369)\n",
            "Saving model (epoch =  929, val loss = 1.5647, train loss = 1.2292)\n",
            "Saving model (epoch =  933, val loss = 1.5646, train loss = 1.4980)\n",
            "Saving model (epoch =  936, val loss = 1.5641, train loss = 1.3064)\n",
            "Saving model (epoch =  937, val loss = 1.5640, train loss = 1.3401)\n",
            "Saving model (epoch =  942, val loss = 1.5638, train loss = 1.4600)\n",
            "Saving model (epoch =  944, val loss = 1.5633, train loss = 1.2038)\n",
            "Saving model (epoch =  952, val loss = 1.5630, train loss = 1.2997)\n",
            "Saving model (epoch =  954, val loss = 1.5626, train loss = 1.2620)\n",
            "Saving model (epoch =  961, val loss = 1.5626, train loss = 1.3534)\n",
            "Saving model (epoch =  963, val loss = 1.5626, train loss = 1.3279)\n",
            "Saving model (epoch =  964, val loss = 1.5625, train loss = 1.3394)\n",
            "Saving model (epoch =  967, val loss = 1.5621, train loss = 1.4167)\n",
            "Saving model (epoch =  970, val loss = 1.5616, train loss = 1.3582)\n",
            "Saving model (epoch =  972, val loss = 1.5613, train loss = 1.4653)\n",
            "Saving model (epoch =  979, val loss = 1.5613, train loss = 1.4382)\n",
            "Saving model (epoch =  985, val loss = 1.5608, train loss = 1.3421)\n",
            "Saving model (epoch =  986, val loss = 1.5608, train loss = 1.2821)\n",
            "Saving model (epoch =  988, val loss = 1.5606, train loss = 1.4759)\n",
            "Saving model (epoch =  992, val loss = 1.5599, train loss = 1.1538)\n",
            "Saving model (epoch =  999, val loss = 1.5599, train loss = 1.3348)\n",
            "Saving model (epoch = 1000, val loss = 1.5597, train loss = 1.5835)\n",
            "Saving model (epoch = 1002, val loss = 1.5593, train loss = 1.2835)\n",
            "Saving model (epoch = 1007, val loss = 1.5592, train loss = 1.4533)\n",
            "Saving model (epoch = 1014, val loss = 1.5592, train loss = 1.2969)\n",
            "Saving model (epoch = 1015, val loss = 1.5588, train loss = 1.2677)\n",
            "Saving model (epoch = 1018, val loss = 1.5584, train loss = 1.3206)\n",
            "Saving model (epoch = 1024, val loss = 1.5583, train loss = 1.3782)\n",
            "Saving model (epoch = 1025, val loss = 1.5578, train loss = 1.3602)\n",
            "Saving model (epoch = 1035, val loss = 1.5576, train loss = 1.1237)\n",
            "Saving model (epoch = 1036, val loss = 1.5574, train loss = 1.3813)\n",
            "Saving model (epoch = 1040, val loss = 1.5572, train loss = 1.4614)\n",
            "Saving model (epoch = 1050, val loss = 1.5566, train loss = 1.3109)\n",
            "Saving model (epoch = 1053, val loss = 1.5565, train loss = 1.3110)\n",
            "Saving model (epoch = 1057, val loss = 1.5564, train loss = 1.4671)\n",
            "Saving model (epoch = 1058, val loss = 1.5563, train loss = 1.5005)\n",
            "Saving model (epoch = 1062, val loss = 1.5558, train loss = 1.2192)\n",
            "Saving model (epoch = 1066, val loss = 1.5555, train loss = 1.4420)\n",
            "Saving model (epoch = 1076, val loss = 1.5553, train loss = 1.4702)\n",
            "Saving model (epoch = 1079, val loss = 1.5550, train loss = 1.3685)\n",
            "Saving model (epoch = 1080, val loss = 1.5550, train loss = 1.3062)\n",
            "Saving model (epoch = 1083, val loss = 1.5548, train loss = 1.3784)\n",
            "Saving model (epoch = 1086, val loss = 1.5544, train loss = 1.2871)\n",
            "Saving model (epoch = 1090, val loss = 1.5542, train loss = 1.3322)\n",
            "Saving model (epoch = 1096, val loss = 1.5540, train loss = 1.4329)\n",
            "Saving model (epoch = 1100, val loss = 1.5535, train loss = 1.3592)\n",
            "Saving model (epoch = 1101, val loss = 1.5535, train loss = 1.3356)\n",
            "Saving model (epoch = 1104, val loss = 1.5533, train loss = 1.2964)\n",
            "Saving model (epoch = 1112, val loss = 1.5531, train loss = 1.4406)\n",
            "Saving model (epoch = 1113, val loss = 1.5530, train loss = 1.3892)\n",
            "Saving model (epoch = 1116, val loss = 1.5529, train loss = 1.4306)\n",
            "Saving model (epoch = 1120, val loss = 1.5526, train loss = 1.6178)\n",
            "Saving model (epoch = 1121, val loss = 1.5525, train loss = 1.4173)\n",
            "Saving model (epoch = 1125, val loss = 1.5524, train loss = 1.3571)\n",
            "Saving model (epoch = 1126, val loss = 1.5523, train loss = 1.3500)\n",
            "Saving model (epoch = 1129, val loss = 1.5522, train loss = 1.2575)\n",
            "Saving model (epoch = 1132, val loss = 1.5521, train loss = 1.4214)\n",
            "Saving model (epoch = 1133, val loss = 1.5519, train loss = 1.3055)\n",
            "Saving model (epoch = 1136, val loss = 1.5519, train loss = 1.4524)\n",
            "Saving model (epoch = 1139, val loss = 1.5516, train loss = 1.5176)\n",
            "Saving model (epoch = 1143, val loss = 1.5514, train loss = 1.3007)\n",
            "Saving model (epoch = 1146, val loss = 1.5513, train loss = 1.3720)\n",
            "Saving model (epoch = 1150, val loss = 1.5511, train loss = 1.1420)\n",
            "Saving model (epoch = 1154, val loss = 1.5506, train loss = 1.5596)\n",
            "Saving model (epoch = 1157, val loss = 1.5505, train loss = 1.3116)\n",
            "Saving model (epoch = 1161, val loss = 1.5501, train loss = 1.4346)\n",
            "Saving model (epoch = 1168, val loss = 1.5498, train loss = 1.2838)\n",
            "Saving model (epoch = 1174, val loss = 1.5496, train loss = 1.4660)\n",
            "Saving model (epoch = 1177, val loss = 1.5494, train loss = 1.2277)\n",
            "Saving model (epoch = 1179, val loss = 1.5492, train loss = 1.2964)\n",
            "Saving model (epoch = 1186, val loss = 1.5491, train loss = 1.2044)\n",
            "Saving model (epoch = 1187, val loss = 1.5488, train loss = 1.2823)\n",
            "Saving model (epoch = 1188, val loss = 1.5487, train loss = 1.2795)\n",
            "Saving model (epoch = 1200, val loss = 1.5484, train loss = 1.2332)\n",
            "Saving model (epoch = 1201, val loss = 1.5481, train loss = 1.4246)\n",
            "Saving model (epoch = 1211, val loss = 1.5473, train loss = 1.5187)\n",
            "Saving model (epoch = 1219, val loss = 1.5472, train loss = 1.2630)\n",
            "Saving model (epoch = 1224, val loss = 1.5469, train loss = 1.3202)\n",
            "Saving model (epoch = 1228, val loss = 1.5469, train loss = 1.2694)\n",
            "Saving model (epoch = 1230, val loss = 1.5466, train loss = 1.3745)\n",
            "Saving model (epoch = 1234, val loss = 1.5464, train loss = 1.3827)\n",
            "Saving model (epoch = 1242, val loss = 1.5455, train loss = 1.3696)\n",
            "Saving model (epoch = 1252, val loss = 1.5454, train loss = 1.4401)\n",
            "Saving model (epoch = 1258, val loss = 1.5451, train loss = 1.4327)\n",
            "Saving model (epoch = 1261, val loss = 1.5449, train loss = 1.3135)\n",
            "Saving model (epoch = 1266, val loss = 1.5449, train loss = 1.4756)\n",
            "Saving model (epoch = 1272, val loss = 1.5446, train loss = 1.3212)\n",
            "Saving model (epoch = 1273, val loss = 1.5446, train loss = 1.3325)\n",
            "Saving model (epoch = 1276, val loss = 1.5445, train loss = 1.5534)\n",
            "Saving model (epoch = 1279, val loss = 1.5444, train loss = 1.3615)\n",
            "Saving model (epoch = 1280, val loss = 1.5436, train loss = 1.5219)\n",
            "Saving model (epoch = 1291, val loss = 1.5433, train loss = 1.2470)\n",
            "Saving model (epoch = 1295, val loss = 1.5432, train loss = 1.4066)\n",
            "Saving model (epoch = 1302, val loss = 1.5432, train loss = 1.3354)\n",
            "Saving model (epoch = 1308, val loss = 1.5428, train loss = 1.2451)\n",
            "Saving model (epoch = 1309, val loss = 1.5424, train loss = 1.4099)\n",
            "Saving model (epoch = 1318, val loss = 1.5422, train loss = 1.3641)\n",
            "Saving model (epoch = 1321, val loss = 1.5419, train loss = 1.6448)\n",
            "Saving model (epoch = 1325, val loss = 1.5419, train loss = 1.3284)\n",
            "Saving model (epoch = 1330, val loss = 1.5414, train loss = 1.4589)\n",
            "Saving model (epoch = 1340, val loss = 1.5413, train loss = 1.3507)\n",
            "Saving model (epoch = 1342, val loss = 1.5411, train loss = 1.4571)\n",
            "Saving model (epoch = 1346, val loss = 1.5411, train loss = 1.2803)\n",
            "Saving model (epoch = 1347, val loss = 1.5410, train loss = 1.3619)\n",
            "Saving model (epoch = 1349, val loss = 1.5407, train loss = 1.4103)\n",
            "Saving model (epoch = 1355, val loss = 1.5404, train loss = 1.3344)\n",
            "Saving model (epoch = 1356, val loss = 1.5403, train loss = 1.3094)\n",
            "Saving model (epoch = 1364, val loss = 1.5400, train loss = 1.2538)\n",
            "Saving model (epoch = 1370, val loss = 1.5399, train loss = 1.3893)\n",
            "Saving model (epoch = 1374, val loss = 1.5395, train loss = 1.2539)\n",
            "Saving model (epoch = 1380, val loss = 1.5393, train loss = 1.3219)\n",
            "Saving model (epoch = 1386, val loss = 1.5393, train loss = 1.2467)\n",
            "Saving model (epoch = 1387, val loss = 1.5392, train loss = 1.5681)\n",
            "Saving model (epoch = 1388, val loss = 1.5391, train loss = 1.3303)\n",
            "Saving model (epoch = 1389, val loss = 1.5390, train loss = 1.2451)\n",
            "Saving model (epoch = 1391, val loss = 1.5390, train loss = 1.4194)\n",
            "Saving model (epoch = 1394, val loss = 1.5388, train loss = 1.2117)\n",
            "Saving model (epoch = 1396, val loss = 1.5382, train loss = 1.4335)\n",
            "Saving model (epoch = 1409, val loss = 1.5381, train loss = 1.2611)\n",
            "Saving model (epoch = 1411, val loss = 1.5374, train loss = 1.2882)\n",
            "Saving model (epoch = 1415, val loss = 1.5373, train loss = 1.2535)\n",
            "Saving model (epoch = 1429, val loss = 1.5369, train loss = 1.3540)\n",
            "Saving model (epoch = 1431, val loss = 1.5369, train loss = 1.2805)\n",
            "Saving model (epoch = 1434, val loss = 1.5363, train loss = 1.2578)\n",
            "Saving model (epoch = 1442, val loss = 1.5361, train loss = 1.3502)\n",
            "Saving model (epoch = 1453, val loss = 1.5358, train loss = 1.2117)\n",
            "Saving model (epoch = 1454, val loss = 1.5355, train loss = 1.3733)\n",
            "Saving model (epoch = 1456, val loss = 1.5353, train loss = 1.3127)\n",
            "Saving model (epoch = 1464, val loss = 1.5352, train loss = 1.1964)\n",
            "Saving model (epoch = 1470, val loss = 1.5350, train loss = 1.2715)\n",
            "Saving model (epoch = 1475, val loss = 1.5348, train loss = 1.1999)\n",
            "Saving model (epoch = 1476, val loss = 1.5348, train loss = 1.4117)\n",
            "Saving model (epoch = 1479, val loss = 1.5345, train loss = 1.1708)\n",
            "Saving model (epoch = 1483, val loss = 1.5344, train loss = 1.4349)\n",
            "Saving model (epoch = 1490, val loss = 1.5343, train loss = 1.3261)\n",
            "Saving model (epoch = 1493, val loss = 1.5342, train loss = 1.4056)\n",
            "Saving model (epoch = 1494, val loss = 1.5339, train loss = 1.3187)\n",
            "Saving model (epoch = 1499, val loss = 1.5337, train loss = 1.1762)\n",
            "Saving model (epoch = 1507, val loss = 1.5333, train loss = 1.4494)\n",
            "Saving model (epoch = 1510, val loss = 1.5333, train loss = 1.2475)\n",
            "Saving model (epoch = 1517, val loss = 1.5329, train loss = 1.6408)\n",
            "Saving model (epoch = 1521, val loss = 1.5329, train loss = 1.3597)\n",
            "Saving model (epoch = 1522, val loss = 1.5326, train loss = 1.4067)\n",
            "Saving model (epoch = 1528, val loss = 1.5323, train loss = 1.3119)\n",
            "Saving model (epoch = 1536, val loss = 1.5321, train loss = 1.3333)\n",
            "Saving model (epoch = 1539, val loss = 1.5321, train loss = 1.4358)\n",
            "Saving model (epoch = 1541, val loss = 1.5319, train loss = 1.3434)\n",
            "Saving model (epoch = 1545, val loss = 1.5317, train loss = 1.3459)\n",
            "Saving model (epoch = 1549, val loss = 1.5315, train loss = 1.5651)\n",
            "Saving model (epoch = 1554, val loss = 1.5314, train loss = 1.3902)\n",
            "Saving model (epoch = 1557, val loss = 1.5311, train loss = 1.2858)\n",
            "Saving model (epoch = 1560, val loss = 1.5309, train loss = 1.3810)\n",
            "Saving model (epoch = 1566, val loss = 1.5304, train loss = 1.4125)\n",
            "Saving model (epoch = 1580, val loss = 1.5303, train loss = 1.3075)\n",
            "Saving model (epoch = 1581, val loss = 1.5301, train loss = 1.4262)\n",
            "Saving model (epoch = 1588, val loss = 1.5295, train loss = 1.3038)\n",
            "Saving model (epoch = 1592, val loss = 1.5294, train loss = 1.4088)\n",
            "Saving model (epoch = 1595, val loss = 1.5294, train loss = 1.2804)\n",
            "Saving model (epoch = 1602, val loss = 1.5291, train loss = 1.3533)\n",
            "Saving model (epoch = 1608, val loss = 1.5291, train loss = 1.1669)\n",
            "Saving model (epoch = 1611, val loss = 1.5289, train loss = 1.2942)\n",
            "Saving model (epoch = 1612, val loss = 1.5284, train loss = 1.2442)\n",
            "Saving model (epoch = 1621, val loss = 1.5283, train loss = 1.2077)\n",
            "Saving model (epoch = 1628, val loss = 1.5281, train loss = 1.5497)\n",
            "Saving model (epoch = 1632, val loss = 1.5278, train loss = 1.1254)\n",
            "Saving model (epoch = 1639, val loss = 1.5276, train loss = 1.2794)\n",
            "Saving model (epoch = 1640, val loss = 1.5272, train loss = 1.2080)\n",
            "Saving model (epoch = 1652, val loss = 1.5272, train loss = 1.3034)\n",
            "Saving model (epoch = 1655, val loss = 1.5267, train loss = 1.3974)\n",
            "Saving model (epoch = 1671, val loss = 1.5265, train loss = 1.2956)\n",
            "Saving model (epoch = 1675, val loss = 1.5262, train loss = 1.3971)\n",
            "Saving model (epoch = 1679, val loss = 1.5260, train loss = 1.4185)\n",
            "Saving model (epoch = 1680, val loss = 1.5258, train loss = 1.4815)\n",
            "Saving model (epoch = 1686, val loss = 1.5258, train loss = 1.2083)\n",
            "Saving model (epoch = 1687, val loss = 1.5257, train loss = 1.3572)\n",
            "Saving model (epoch = 1691, val loss = 1.5253, train loss = 1.4109)\n",
            "Saving model (epoch = 1697, val loss = 1.5251, train loss = 1.3059)\n",
            "Saving model (epoch = 1701, val loss = 1.5250, train loss = 1.2604)\n",
            "Saving model (epoch = 1707, val loss = 1.5246, train loss = 1.1731)\n",
            "Saving model (epoch = 1717, val loss = 1.5243, train loss = 1.2445)\n",
            "Saving model (epoch = 1721, val loss = 1.5242, train loss = 1.2740)\n",
            "Saving model (epoch = 1724, val loss = 1.5242, train loss = 1.3926)\n",
            "Saving model (epoch = 1733, val loss = 1.5238, train loss = 1.4828)\n",
            "Saving model (epoch = 1736, val loss = 1.5234, train loss = 1.2786)\n",
            "Saving model (epoch = 1745, val loss = 1.5232, train loss = 1.3789)\n",
            "Saving model (epoch = 1749, val loss = 1.5231, train loss = 1.3063)\n",
            "Saving model (epoch = 1752, val loss = 1.5229, train loss = 1.2930)\n",
            "Saving model (epoch = 1756, val loss = 1.5228, train loss = 1.2834)\n",
            "Saving model (epoch = 1760, val loss = 1.5225, train loss = 1.3972)\n",
            "Saving model (epoch = 1764, val loss = 1.5225, train loss = 1.3993)\n",
            "Saving model (epoch = 1769, val loss = 1.5222, train loss = 1.4037)\n",
            "Saving model (epoch = 1772, val loss = 1.5220, train loss = 1.5256)\n",
            "Saving model (epoch = 1777, val loss = 1.5219, train loss = 1.2523)\n",
            "Saving model (epoch = 1788, val loss = 1.5215, train loss = 1.3275)\n",
            "Saving model (epoch = 1794, val loss = 1.5213, train loss = 1.3286)\n",
            "Saving model (epoch = 1802, val loss = 1.5211, train loss = 1.4146)\n",
            "Saving model (epoch = 1804, val loss = 1.5211, train loss = 1.3998)\n",
            "Saving model (epoch = 1805, val loss = 1.5210, train loss = 1.1116)\n",
            "Saving model (epoch = 1808, val loss = 1.5210, train loss = 1.2177)\n",
            "Saving model (epoch = 1809, val loss = 1.5207, train loss = 1.2883)\n",
            "Saving model (epoch = 1815, val loss = 1.5203, train loss = 1.2695)\n",
            "Saving model (epoch = 1830, val loss = 1.5202, train loss = 1.4005)\n",
            "Saving model (epoch = 1833, val loss = 1.5200, train loss = 1.3001)\n",
            "Saving model (epoch = 1834, val loss = 1.5197, train loss = 1.2229)\n",
            "Saving model (epoch = 1844, val loss = 1.5196, train loss = 1.3057)\n",
            "Saving model (epoch = 1846, val loss = 1.5193, train loss = 1.3204)\n",
            "Saving model (epoch = 1850, val loss = 1.5192, train loss = 1.3082)\n",
            "Saving model (epoch = 1852, val loss = 1.5192, train loss = 1.2508)\n",
            "Saving model (epoch = 1855, val loss = 1.5190, train loss = 1.2720)\n",
            "Saving model (epoch = 1862, val loss = 1.5186, train loss = 1.1840)\n",
            "Saving model (epoch = 1868, val loss = 1.5185, train loss = 1.1925)\n",
            "Saving model (epoch = 1873, val loss = 1.5185, train loss = 1.3241)\n",
            "Saving model (epoch = 1875, val loss = 1.5182, train loss = 1.2780)\n",
            "Saving model (epoch = 1876, val loss = 1.5177, train loss = 1.4095)\n",
            "Saving model (epoch = 1889, val loss = 1.5176, train loss = 1.1501)\n",
            "Saving model (epoch = 1893, val loss = 1.5176, train loss = 1.3960)\n",
            "Saving model (epoch = 1897, val loss = 1.5175, train loss = 1.4338)\n",
            "Saving model (epoch = 1898, val loss = 1.5174, train loss = 1.2920)\n",
            "Saving model (epoch = 1902, val loss = 1.5169, train loss = 1.1230)\n",
            "Saving model (epoch = 1912, val loss = 1.5166, train loss = 1.3098)\n",
            "Saving model (epoch = 1916, val loss = 1.5165, train loss = 1.2020)\n",
            "Saving model (epoch = 1924, val loss = 1.5164, train loss = 1.3727)\n",
            "Saving model (epoch = 1927, val loss = 1.5159, train loss = 1.3792)\n",
            "Saving model (epoch = 1943, val loss = 1.5157, train loss = 1.4268)\n",
            "Saving model (epoch = 1948, val loss = 1.5156, train loss = 1.4462)\n",
            "Saving model (epoch = 1950, val loss = 1.5154, train loss = 1.3274)\n",
            "Saving model (epoch = 1954, val loss = 1.5153, train loss = 1.2769)\n",
            "Saving model (epoch = 1959, val loss = 1.5150, train loss = 1.2297)\n",
            "Saving model (epoch = 1968, val loss = 1.5149, train loss = 1.2823)\n",
            "Saving model (epoch = 1969, val loss = 1.5147, train loss = 1.3051)\n",
            "Saving model (epoch = 1977, val loss = 1.5140, train loss = 1.2287)\n",
            "Saving model (epoch = 1984, val loss = 1.5138, train loss = 1.3717)\n",
            "Saving model (epoch = 1994, val loss = 1.5133, train loss = 1.3709)\n",
            "Saving model (epoch = 2010, val loss = 1.5130, train loss = 1.4398)\n",
            "Saving model (epoch = 2019, val loss = 1.5126, train loss = 1.1517)\n",
            "Saving model (epoch = 2026, val loss = 1.5125, train loss = 1.2969)\n",
            "Saving model (epoch = 2029, val loss = 1.5124, train loss = 1.4161)\n",
            "Saving model (epoch = 2036, val loss = 1.5123, train loss = 1.2960)\n",
            "Saving model (epoch = 2039, val loss = 1.5119, train loss = 1.2013)\n",
            "Saving model (epoch = 2050, val loss = 1.5115, train loss = 1.3430)\n",
            "Saving model (epoch = 2061, val loss = 1.5114, train loss = 1.3725)\n",
            "Saving model (epoch = 2062, val loss = 1.5114, train loss = 1.3313)\n",
            "Saving model (epoch = 2066, val loss = 1.5113, train loss = 1.2952)\n",
            "Saving model (epoch = 2069, val loss = 1.5111, train loss = 1.4152)\n",
            "Saving model (epoch = 2073, val loss = 1.5107, train loss = 1.3531)\n",
            "Saving model (epoch = 2089, val loss = 1.5103, train loss = 1.1172)\n",
            "Saving model (epoch = 2096, val loss = 1.5103, train loss = 1.4173)\n",
            "Saving model (epoch = 2099, val loss = 1.5102, train loss = 1.4278)\n",
            "Saving model (epoch = 2104, val loss = 1.5098, train loss = 1.3199)\n",
            "Saving model (epoch = 2108, val loss = 1.5094, train loss = 1.2073)\n",
            "Saving model (epoch = 2119, val loss = 1.5092, train loss = 1.1376)\n",
            "Saving model (epoch = 2122, val loss = 1.5089, train loss = 1.3762)\n",
            "Saving model (epoch = 2137, val loss = 1.5086, train loss = 1.2975)\n",
            "Saving model (epoch = 2144, val loss = 1.5083, train loss = 1.2627)\n",
            "Saving model (epoch = 2155, val loss = 1.5080, train loss = 1.1537)\n",
            "Saving model (epoch = 2165, val loss = 1.5080, train loss = 1.3048)\n",
            "Saving model (epoch = 2168, val loss = 1.5075, train loss = 1.1204)\n",
            "Saving model (epoch = 2184, val loss = 1.5072, train loss = 1.2538)\n",
            "Saving model (epoch = 2185, val loss = 1.5070, train loss = 1.2248)\n",
            "Saving model (epoch = 2194, val loss = 1.5067, train loss = 1.3598)\n",
            "Saving model (epoch = 2197, val loss = 1.5066, train loss = 1.5289)\n",
            "Saving model (epoch = 2201, val loss = 1.5065, train loss = 1.3458)\n",
            "Saving model (epoch = 2207, val loss = 1.5065, train loss = 1.3918)\n",
            "Saving model (epoch = 2214, val loss = 1.5059, train loss = 1.2586)\n",
            "Saving model (epoch = 2223, val loss = 1.5059, train loss = 1.4776)\n",
            "Saving model (epoch = 2224, val loss = 1.5058, train loss = 1.3718)\n",
            "Saving model (epoch = 2234, val loss = 1.5056, train loss = 1.3955)\n",
            "Saving model (epoch = 2235, val loss = 1.5053, train loss = 1.3650)\n",
            "Saving model (epoch = 2244, val loss = 1.5052, train loss = 1.2234)\n",
            "Saving model (epoch = 2249, val loss = 1.5050, train loss = 1.3990)\n",
            "Saving model (epoch = 2257, val loss = 1.5050, train loss = 1.1992)\n",
            "Saving model (epoch = 2259, val loss = 1.5047, train loss = 1.5919)\n",
            "Saving model (epoch = 2263, val loss = 1.5044, train loss = 1.1831)\n",
            "Saving model (epoch = 2271, val loss = 1.5044, train loss = 1.2679)\n",
            "Saving model (epoch = 2276, val loss = 1.5041, train loss = 1.5142)\n",
            "Saving model (epoch = 2278, val loss = 1.5040, train loss = 1.3017)\n",
            "Saving model (epoch = 2291, val loss = 1.5039, train loss = 1.3486)\n",
            "Saving model (epoch = 2296, val loss = 1.5036, train loss = 1.2313)\n",
            "Saving model (epoch = 2298, val loss = 1.5032, train loss = 1.1564)\n",
            "Saving model (epoch = 2301, val loss = 1.5032, train loss = 1.1814)\n",
            "Saving model (epoch = 2313, val loss = 1.5028, train loss = 1.3506)\n",
            "Saving model (epoch = 2320, val loss = 1.5027, train loss = 1.2808)\n",
            "Saving model (epoch = 2329, val loss = 1.5023, train loss = 1.1519)\n",
            "Saving model (epoch = 2334, val loss = 1.5023, train loss = 1.4469)\n",
            "Saving model (epoch = 2335, val loss = 1.5022, train loss = 1.3153)\n",
            "Saving model (epoch = 2345, val loss = 1.5021, train loss = 1.2000)\n",
            "Saving model (epoch = 2346, val loss = 1.5018, train loss = 1.0861)\n",
            "Saving model (epoch = 2352, val loss = 1.5015, train loss = 1.2503)\n",
            "Saving model (epoch = 2366, val loss = 1.5011, train loss = 1.2504)\n",
            "Saving model (epoch = 2367, val loss = 1.5009, train loss = 1.1911)\n",
            "Saving model (epoch = 2384, val loss = 1.5005, train loss = 1.2507)\n",
            "Saving model (epoch = 2393, val loss = 1.5003, train loss = 1.3601)\n",
            "Saving model (epoch = 2408, val loss = 1.5001, train loss = 1.3504)\n",
            "Saving model (epoch = 2409, val loss = 1.5001, train loss = 1.2362)\n",
            "Saving model (epoch = 2410, val loss = 1.4997, train loss = 1.3076)\n",
            "Saving model (epoch = 2417, val loss = 1.4996, train loss = 1.2901)\n",
            "Saving model (epoch = 2425, val loss = 1.4995, train loss = 1.2901)\n",
            "Saving model (epoch = 2429, val loss = 1.4993, train loss = 1.2354)\n",
            "Saving model (epoch = 2436, val loss = 1.4991, train loss = 1.2409)\n",
            "Saving model (epoch = 2438, val loss = 1.4991, train loss = 1.5137)\n",
            "Saving model (epoch = 2442, val loss = 1.4990, train loss = 1.3326)\n",
            "Saving model (epoch = 2444, val loss = 1.4990, train loss = 1.2622)\n",
            "Saving model (epoch = 2446, val loss = 1.4990, train loss = 1.3863)\n",
            "Saving model (epoch = 2448, val loss = 1.4988, train loss = 1.3178)\n",
            "Saving model (epoch = 2451, val loss = 1.4987, train loss = 1.3522)\n",
            "Saving model (epoch = 2453, val loss = 1.4987, train loss = 1.3475)\n",
            "Saving model (epoch = 2454, val loss = 1.4987, train loss = 1.3603)\n",
            "Saving model (epoch = 2458, val loss = 1.4986, train loss = 1.2892)\n",
            "Saving model (epoch = 2460, val loss = 1.4984, train loss = 1.3169)\n",
            "Saving model (epoch = 2464, val loss = 1.4982, train loss = 1.4256)\n",
            "Saving model (epoch = 2468, val loss = 1.4982, train loss = 1.2759)\n",
            "Saving model (epoch = 2472, val loss = 1.4981, train loss = 1.3075)\n",
            "Saving model (epoch = 2477, val loss = 1.4981, train loss = 1.3061)\n",
            "Saving model (epoch = 2479, val loss = 1.4979, train loss = 1.3976)\n",
            "Saving model (epoch = 2480, val loss = 1.4977, train loss = 1.2753)\n",
            "Saving model (epoch = 2483, val loss = 1.4977, train loss = 1.1007)\n",
            "Saving model (epoch = 2485, val loss = 1.4977, train loss = 1.4032)\n",
            "Saving model (epoch = 2487, val loss = 1.4974, train loss = 1.1319)\n",
            "Saving model (epoch = 2497, val loss = 1.4974, train loss = 1.3296)\n",
            "Saving model (epoch = 2503, val loss = 1.4972, train loss = 1.3340)\n",
            "Saving model (epoch = 2505, val loss = 1.4969, train loss = 1.1507)\n",
            "Saving model (epoch = 2511, val loss = 1.4969, train loss = 1.3173)\n",
            "Saving model (epoch = 2512, val loss = 1.4963, train loss = 1.1790)\n",
            "Saving model (epoch = 2530, val loss = 1.4962, train loss = 1.3735)\n",
            "Saving model (epoch = 2532, val loss = 1.4960, train loss = 1.0105)\n",
            "Saving model (epoch = 2537, val loss = 1.4959, train loss = 1.2570)\n",
            "Saving model (epoch = 2541, val loss = 1.4959, train loss = 1.1977)\n",
            "Saving model (epoch = 2547, val loss = 1.4953, train loss = 1.2888)\n",
            "Saving model (epoch = 2559, val loss = 1.4952, train loss = 1.1035)\n",
            "Saving model (epoch = 2564, val loss = 1.4951, train loss = 1.3680)\n",
            "Saving model (epoch = 2571, val loss = 1.4951, train loss = 1.1666)\n",
            "Saving model (epoch = 2572, val loss = 1.4949, train loss = 1.3222)\n",
            "Saving model (epoch = 2577, val loss = 1.4947, train loss = 1.4752)\n",
            "Saving model (epoch = 2581, val loss = 1.4943, train loss = 1.3492)\n",
            "Saving model (epoch = 2589, val loss = 1.4943, train loss = 1.3618)\n",
            "Saving model (epoch = 2598, val loss = 1.4942, train loss = 1.4175)\n",
            "Saving model (epoch = 2601, val loss = 1.4936, train loss = 1.3260)\n",
            "Saving model (epoch = 2613, val loss = 1.4936, train loss = 1.3142)\n",
            "Saving model (epoch = 2621, val loss = 1.4935, train loss = 1.2418)\n",
            "Saving model (epoch = 2625, val loss = 1.4932, train loss = 1.2571)\n",
            "Saving model (epoch = 2626, val loss = 1.4930, train loss = 1.3058)\n",
            "Saving model (epoch = 2642, val loss = 1.4930, train loss = 1.4336)\n",
            "Saving model (epoch = 2644, val loss = 1.4927, train loss = 1.2932)\n",
            "Saving model (epoch = 2651, val loss = 1.4925, train loss = 1.4086)\n",
            "Saving model (epoch = 2654, val loss = 1.4923, train loss = 1.3726)\n",
            "Saving model (epoch = 2659, val loss = 1.4921, train loss = 1.3787)\n",
            "Saving model (epoch = 2672, val loss = 1.4915, train loss = 1.4526)\n",
            "Saving model (epoch = 2688, val loss = 1.4914, train loss = 1.3178)\n",
            "Saving model (epoch = 2693, val loss = 1.4914, train loss = 1.3361)\n",
            "Saving model (epoch = 2701, val loss = 1.4911, train loss = 1.3886)\n",
            "Saving model (epoch = 2708, val loss = 1.4909, train loss = 1.3224)\n",
            "Saving model (epoch = 2714, val loss = 1.4907, train loss = 1.3535)\n",
            "Saving model (epoch = 2719, val loss = 1.4906, train loss = 1.4763)\n",
            "Saving model (epoch = 2727, val loss = 1.4899, train loss = 1.2996)\n",
            "Saving model (epoch = 2739, val loss = 1.4897, train loss = 1.3470)\n",
            "Saving model (epoch = 2740, val loss = 1.4897, train loss = 1.1865)\n",
            "Saving model (epoch = 2753, val loss = 1.4894, train loss = 1.4443)\n",
            "Saving model (epoch = 2759, val loss = 1.4893, train loss = 1.4059)\n",
            "Saving model (epoch = 2765, val loss = 1.4893, train loss = 1.2536)\n",
            "Saving model (epoch = 2770, val loss = 1.4892, train loss = 1.5385)\n",
            "Saving model (epoch = 2774, val loss = 1.4890, train loss = 1.3041)\n",
            "Saving model (epoch = 2776, val loss = 1.4889, train loss = 1.2885)\n",
            "Saving model (epoch = 2780, val loss = 1.4886, train loss = 1.4019)\n",
            "Saving model (epoch = 2781, val loss = 1.4886, train loss = 1.2358)\n",
            "Saving model (epoch = 2792, val loss = 1.4882, train loss = 1.1441)\n",
            "Saving model (epoch = 2796, val loss = 1.4882, train loss = 1.2324)\n",
            "Saving model (epoch = 2800, val loss = 1.4882, train loss = 1.3420)\n",
            "Saving model (epoch = 2802, val loss = 1.4881, train loss = 1.3456)\n",
            "Saving model (epoch = 2804, val loss = 1.4880, train loss = 1.3478)\n",
            "Saving model (epoch = 2813, val loss = 1.4879, train loss = 1.2711)\n",
            "Saving model (epoch = 2815, val loss = 1.4877, train loss = 1.1774)\n",
            "Saving model (epoch = 2818, val loss = 1.4876, train loss = 1.5658)\n",
            "Saving model (epoch = 2831, val loss = 1.4872, train loss = 1.2496)\n",
            "Saving model (epoch = 2832, val loss = 1.4871, train loss = 1.2269)\n",
            "Saving model (epoch = 2839, val loss = 1.4868, train loss = 1.3042)\n",
            "Saving model (epoch = 2847, val loss = 1.4867, train loss = 1.2300)\n",
            "Saving model (epoch = 2856, val loss = 1.4865, train loss = 1.1876)\n",
            "Saving model (epoch = 2863, val loss = 1.4863, train loss = 1.2813)\n",
            "Saving model (epoch = 2874, val loss = 1.4861, train loss = 1.1501)\n",
            "Saving model (epoch = 2875, val loss = 1.4860, train loss = 1.3572)\n",
            "Saving model (epoch = 2881, val loss = 1.4860, train loss = 1.2954)\n",
            "Saving model (epoch = 2886, val loss = 1.4856, train loss = 1.1061)\n",
            "Saving model (epoch = 2893, val loss = 1.4854, train loss = 1.2933)\n",
            "Saving model (epoch = 2898, val loss = 1.4853, train loss = 1.1525)\n",
            "Saving model (epoch = 2906, val loss = 1.4853, train loss = 1.3596)\n",
            "Saving model (epoch = 2907, val loss = 1.4851, train loss = 1.2479)\n",
            "Saving model (epoch = 2911, val loss = 1.4851, train loss = 1.2485)\n",
            "Saving model (epoch = 2913, val loss = 1.4847, train loss = 1.1206)\n",
            "Saving model (epoch = 2916, val loss = 1.4847, train loss = 1.4137)\n",
            "Saving model (epoch = 2924, val loss = 1.4847, train loss = 1.3028)\n",
            "Saving model (epoch = 2932, val loss = 1.4847, train loss = 1.2262)\n",
            "Saving model (epoch = 2936, val loss = 1.4844, train loss = 1.3603)\n",
            "Saving model (epoch = 2940, val loss = 1.4844, train loss = 1.1052)\n",
            "Saving model (epoch = 2943, val loss = 1.4839, train loss = 1.3823)\n",
            "Saving model (epoch = 2951, val loss = 1.4839, train loss = 1.3219)\n",
            "Saving model (epoch = 2961, val loss = 1.4838, train loss = 1.4374)\n",
            "Saving model (epoch = 2962, val loss = 1.4832, train loss = 1.1883)\n",
            "Saving model (epoch = 2983, val loss = 1.4831, train loss = 1.2933)\n",
            "Saving model (epoch = 2989, val loss = 1.4831, train loss = 1.3546)\n",
            "Saving model (epoch = 2991, val loss = 1.4830, train loss = 1.2419)\n",
            "Saving model (epoch = 2992, val loss = 1.4826, train loss = 1.2371)\n",
            "Saving model (epoch = 3007, val loss = 1.4824, train loss = 1.3085)\n",
            "Saving model (epoch = 3010, val loss = 1.4821, train loss = 1.2516)\n",
            "Saving model (epoch = 3017, val loss = 1.4818, train loss = 1.2414)\n",
            "Saving model (epoch = 3027, val loss = 1.4818, train loss = 1.4726)\n",
            "Saving model (epoch = 3035, val loss = 1.4817, train loss = 1.2567)\n",
            "Saving model (epoch = 3039, val loss = 1.4814, train loss = 1.1965)\n",
            "Saving model (epoch = 3051, val loss = 1.4813, train loss = 1.3553)\n",
            "Saving model (epoch = 3055, val loss = 1.4812, train loss = 1.2973)\n",
            "Saving model (epoch = 3060, val loss = 1.4811, train loss = 1.2202)\n",
            "Saving model (epoch = 3066, val loss = 1.4807, train loss = 1.3207)\n",
            "Saving model (epoch = 3074, val loss = 1.4807, train loss = 1.3130)\n",
            "Saving model (epoch = 3082, val loss = 1.4806, train loss = 1.2289)\n",
            "Saving model (epoch = 3086, val loss = 1.4805, train loss = 1.3978)\n",
            "Saving model (epoch = 3088, val loss = 1.4805, train loss = 1.1881)\n",
            "Saving model (epoch = 3094, val loss = 1.4801, train loss = 1.2631)\n",
            "Saving model (epoch = 3102, val loss = 1.4801, train loss = 1.1831)\n",
            "Saving model (epoch = 3109, val loss = 1.4798, train loss = 1.2739)\n",
            "Saving model (epoch = 3115, val loss = 1.4798, train loss = 1.3418)\n",
            "Saving model (epoch = 3118, val loss = 1.4795, train loss = 1.0562)\n",
            "Saving model (epoch = 3123, val loss = 1.4793, train loss = 1.2887)\n",
            "Saving model (epoch = 3126, val loss = 1.4792, train loss = 1.3439)\n",
            "Saving model (epoch = 3135, val loss = 1.4789, train loss = 1.2501)\n",
            "Saving model (epoch = 3145, val loss = 1.4789, train loss = 1.4141)\n",
            "Saving model (epoch = 3147, val loss = 1.4786, train loss = 1.2016)\n",
            "Saving model (epoch = 3153, val loss = 1.4784, train loss = 1.3017)\n",
            "Saving model (epoch = 3156, val loss = 1.4784, train loss = 1.2031)\n",
            "Saving model (epoch = 3167, val loss = 1.4784, train loss = 1.0650)\n",
            "Saving model (epoch = 3169, val loss = 1.4782, train loss = 1.2992)\n",
            "Saving model (epoch = 3174, val loss = 1.4782, train loss = 1.1506)\n",
            "Saving model (epoch = 3182, val loss = 1.4781, train loss = 1.2688)\n",
            "Saving model (epoch = 3183, val loss = 1.4779, train loss = 1.1254)\n",
            "Saving model (epoch = 3188, val loss = 1.4773, train loss = 1.4112)\n",
            "Saving model (epoch = 3208, val loss = 1.4770, train loss = 1.3288)\n",
            "Saving model (epoch = 3223, val loss = 1.4769, train loss = 1.2315)\n",
            "Saving model (epoch = 3230, val loss = 1.4767, train loss = 1.3200)\n",
            "Saving model (epoch = 3240, val loss = 1.4767, train loss = 1.0350)\n",
            "Saving model (epoch = 3241, val loss = 1.4766, train loss = 1.3890)\n",
            "Saving model (epoch = 3243, val loss = 1.4765, train loss = 1.2230)\n",
            "Saving model (epoch = 3246, val loss = 1.4764, train loss = 1.3133)\n",
            "Saving model (epoch = 3255, val loss = 1.4759, train loss = 1.2311)\n",
            "Saving model (epoch = 3266, val loss = 1.4757, train loss = 1.4130)\n",
            "Saving model (epoch = 3278, val loss = 1.4756, train loss = 1.2517)\n",
            "Saving model (epoch = 3279, val loss = 1.4755, train loss = 1.1924)\n",
            "Saving model (epoch = 3293, val loss = 1.4750, train loss = 1.2279)\n",
            "Saving model (epoch = 3296, val loss = 1.4750, train loss = 1.1193)\n",
            "Saving model (epoch = 3302, val loss = 1.4750, train loss = 1.2812)\n",
            "Saving model (epoch = 3312, val loss = 1.4749, train loss = 1.1484)\n",
            "Saving model (epoch = 3314, val loss = 1.4747, train loss = 1.1104)\n",
            "Saving model (epoch = 3325, val loss = 1.4744, train loss = 1.4139)\n",
            "Saving model (epoch = 3332, val loss = 1.4744, train loss = 1.3065)\n",
            "Saving model (epoch = 3336, val loss = 1.4735, train loss = 1.2991)\n",
            "Saving model (epoch = 3364, val loss = 1.4733, train loss = 1.3540)\n",
            "Saving model (epoch = 3380, val loss = 1.4733, train loss = 1.3513)\n",
            "Saving model (epoch = 3383, val loss = 1.4732, train loss = 1.2623)\n",
            "Saving model (epoch = 3385, val loss = 1.4731, train loss = 1.2084)\n",
            "Saving model (epoch = 3389, val loss = 1.4723, train loss = 1.1652)\n",
            "Saving model (epoch = 3420, val loss = 1.4723, train loss = 1.3657)\n",
            "Saving model (epoch = 3423, val loss = 1.4719, train loss = 1.2076)\n",
            "Saving model (epoch = 3426, val loss = 1.4719, train loss = 1.1621)\n",
            "Saving model (epoch = 3433, val loss = 1.4718, train loss = 1.3414)\n",
            "Saving model (epoch = 3438, val loss = 1.4717, train loss = 1.2502)\n",
            "Saving model (epoch = 3439, val loss = 1.4714, train loss = 1.4860)\n",
            "Saving model (epoch = 3450, val loss = 1.4713, train loss = 1.4045)\n",
            "Saving model (epoch = 3457, val loss = 1.4713, train loss = 1.2497)\n",
            "Saving model (epoch = 3466, val loss = 1.4707, train loss = 1.3803)\n",
            "Saving model (epoch = 3480, val loss = 1.4706, train loss = 1.1522)\n",
            "Saving model (epoch = 3487, val loss = 1.4706, train loss = 1.3998)\n",
            "Saving model (epoch = 3493, val loss = 1.4701, train loss = 1.1222)\n",
            "Saving model (epoch = 3519, val loss = 1.4700, train loss = 1.1588)\n",
            "Saving model (epoch = 3524, val loss = 1.4695, train loss = 1.2229)\n",
            "Saving model (epoch = 3531, val loss = 1.4695, train loss = 1.2647)\n",
            "Saving model (epoch = 3542, val loss = 1.4694, train loss = 1.2516)\n",
            "Saving model (epoch = 3551, val loss = 1.4693, train loss = 1.2453)\n",
            "Saving model (epoch = 3561, val loss = 1.4687, train loss = 1.2657)\n",
            "Saving model (epoch = 3587, val loss = 1.4686, train loss = 1.1399)\n",
            "Saving model (epoch = 3588, val loss = 1.4685, train loss = 1.2924)\n",
            "Saving model (epoch = 3589, val loss = 1.4684, train loss = 1.2217)\n",
            "Saving model (epoch = 3591, val loss = 1.4682, train loss = 1.1839)\n",
            "Saving model (epoch = 3596, val loss = 1.4682, train loss = 1.2857)\n",
            "Saving model (epoch = 3598, val loss = 1.4680, train loss = 1.3564)\n",
            "Saving model (epoch = 3610, val loss = 1.4678, train loss = 1.3647)\n",
            "Saving model (epoch = 3620, val loss = 1.4678, train loss = 1.2447)\n",
            "Saving model (epoch = 3627, val loss = 1.4675, train loss = 1.3577)\n",
            "Saving model (epoch = 3638, val loss = 1.4672, train loss = 1.2571)\n",
            "Saving model (epoch = 3651, val loss = 1.4672, train loss = 1.2617)\n",
            "Saving model (epoch = 3652, val loss = 1.4671, train loss = 1.2865)\n",
            "Saving model (epoch = 3656, val loss = 1.4671, train loss = 1.4054)\n",
            "Saving model (epoch = 3658, val loss = 1.4669, train loss = 1.1660)\n",
            "Saving model (epoch = 3664, val loss = 1.4667, train loss = 1.2053)\n",
            "Saving model (epoch = 3681, val loss = 1.4666, train loss = 1.3437)\n",
            "Saving model (epoch = 3685, val loss = 1.4664, train loss = 1.3893)\n",
            "Saving model (epoch = 3690, val loss = 1.4656, train loss = 1.3009)\n",
            "Saving model (epoch = 3733, val loss = 1.4653, train loss = 1.3227)\n",
            "Saving model (epoch = 3757, val loss = 1.4651, train loss = 1.2883)\n",
            "Saving model (epoch = 3759, val loss = 1.4646, train loss = 1.2465)\n",
            "Saving model (epoch = 3770, val loss = 1.4646, train loss = 1.3117)\n",
            "Saving model (epoch = 3773, val loss = 1.4646, train loss = 1.2086)\n",
            "Saving model (epoch = 3774, val loss = 1.4645, train loss = 1.1875)\n",
            "Saving model (epoch = 3779, val loss = 1.4645, train loss = 1.0813)\n",
            "Saving model (epoch = 3788, val loss = 1.4644, train loss = 1.1836)\n",
            "Saving model (epoch = 3797, val loss = 1.4643, train loss = 1.3674)\n",
            "Saving model (epoch = 3800, val loss = 1.4639, train loss = 1.3121)\n",
            "Saving model (epoch = 3824, val loss = 1.4639, train loss = 1.1357)\n",
            "Saving model (epoch = 3827, val loss = 1.4637, train loss = 1.2581)\n",
            "Saving model (epoch = 3831, val loss = 1.4636, train loss = 1.3438)\n",
            "Saving model (epoch = 3832, val loss = 1.4634, train loss = 1.3081)\n",
            "Saving model (epoch = 3837, val loss = 1.4633, train loss = 1.2745)\n",
            "Saving model (epoch = 3841, val loss = 1.4633, train loss = 1.2363)\n",
            "Saving model (epoch = 3842, val loss = 1.4633, train loss = 1.4028)\n",
            "Saving model (epoch = 3848, val loss = 1.4632, train loss = 1.2046)\n",
            "Saving model (epoch = 3852, val loss = 1.4627, train loss = 1.3163)\n",
            "Saving model (epoch = 3867, val loss = 1.4626, train loss = 1.1897)\n",
            "Saving model (epoch = 3873, val loss = 1.4623, train loss = 1.2363)\n",
            "Saving model (epoch = 3890, val loss = 1.4621, train loss = 1.1068)\n",
            "Saving model (epoch = 3903, val loss = 1.4619, train loss = 1.2253)\n",
            "Saving model (epoch = 3904, val loss = 1.4619, train loss = 1.2359)\n",
            "Saving model (epoch = 3916, val loss = 1.4619, train loss = 1.3473)\n",
            "Saving model (epoch = 3919, val loss = 1.4615, train loss = 1.4649)\n",
            "Saving model (epoch = 3932, val loss = 1.4615, train loss = 1.1933)\n",
            "Saving model (epoch = 3933, val loss = 1.4612, train loss = 1.3656)\n",
            "Saving model (epoch = 3950, val loss = 1.4611, train loss = 1.1949)\n",
            "Saving model (epoch = 3955, val loss = 1.4606, train loss = 1.2104)\n",
            "Saving model (epoch = 3985, val loss = 1.4603, train loss = 1.3462)\n",
            "Saving model (epoch = 3995, val loss = 1.4601, train loss = 1.1802)\n",
            "Saving model (epoch = 4015, val loss = 1.4599, train loss = 1.4331)\n",
            "Saving model (epoch = 4016, val loss = 1.4596, train loss = 1.2925)\n",
            "Saving model (epoch = 4039, val loss = 1.4593, train loss = 1.3056)\n",
            "Saving model (epoch = 4049, val loss = 1.4592, train loss = 1.1611)\n",
            "Saving model (epoch = 4052, val loss = 1.4592, train loss = 1.2691)\n",
            "Saving model (epoch = 4053, val loss = 1.4590, train loss = 1.4168)\n",
            "Saving model (epoch = 4064, val loss = 1.4590, train loss = 1.3102)\n",
            "Saving model (epoch = 4070, val loss = 1.4588, train loss = 1.2353)\n",
            "Saving model (epoch = 4073, val loss = 1.4587, train loss = 1.3079)\n",
            "Saving model (epoch = 4083, val loss = 1.4586, train loss = 1.3352)\n",
            "Saving model (epoch = 4096, val loss = 1.4585, train loss = 1.3278)\n",
            "Saving model (epoch = 4099, val loss = 1.4584, train loss = 1.1550)\n",
            "Saving model (epoch = 4105, val loss = 1.4582, train loss = 1.2140)\n",
            "Saving model (epoch = 4112, val loss = 1.4582, train loss = 1.3328)\n",
            "Saving model (epoch = 4113, val loss = 1.4581, train loss = 1.2818)\n",
            "Saving model (epoch = 4120, val loss = 1.4581, train loss = 1.3769)\n",
            "Saving model (epoch = 4121, val loss = 1.4581, train loss = 1.2681)\n",
            "Saving model (epoch = 4127, val loss = 1.4579, train loss = 1.3980)\n",
            "Saving model (epoch = 4131, val loss = 1.4578, train loss = 1.5212)\n",
            "Saving model (epoch = 4144, val loss = 1.4571, train loss = 1.3203)\n",
            "Saving model (epoch = 4168, val loss = 1.4568, train loss = 1.5096)\n",
            "Saving model (epoch = 4202, val loss = 1.4566, train loss = 1.1777)\n",
            "Saving model (epoch = 4205, val loss = 1.4566, train loss = 1.2937)\n",
            "Saving model (epoch = 4206, val loss = 1.4566, train loss = 1.1687)\n",
            "Saving model (epoch = 4211, val loss = 1.4565, train loss = 1.2833)\n",
            "Saving model (epoch = 4213, val loss = 1.4564, train loss = 1.4151)\n",
            "Saving model (epoch = 4220, val loss = 1.4562, train loss = 1.3377)\n",
            "Saving model (epoch = 4224, val loss = 1.4561, train loss = 1.2021)\n",
            "Saving model (epoch = 4226, val loss = 1.4561, train loss = 1.3936)\n",
            "Saving model (epoch = 4227, val loss = 1.4560, train loss = 1.2077)\n",
            "Saving model (epoch = 4229, val loss = 1.4559, train loss = 1.2596)\n",
            "Saving model (epoch = 4233, val loss = 1.4557, train loss = 1.2951)\n",
            "Saving model (epoch = 4244, val loss = 1.4556, train loss = 1.2444)\n",
            "Saving model (epoch = 4251, val loss = 1.4554, train loss = 1.1414)\n",
            "Saving model (epoch = 4262, val loss = 1.4553, train loss = 1.2212)\n",
            "Saving model (epoch = 4273, val loss = 1.4552, train loss = 1.1036)\n",
            "Saving model (epoch = 4277, val loss = 1.4552, train loss = 1.3756)\n",
            "Saving model (epoch = 4288, val loss = 1.4551, train loss = 1.5958)\n",
            "Saving model (epoch = 4290, val loss = 1.4547, train loss = 1.3810)\n",
            "Saving model (epoch = 4294, val loss = 1.4547, train loss = 1.2338)\n",
            "Saving model (epoch = 4306, val loss = 1.4545, train loss = 1.4779)\n",
            "Saving model (epoch = 4321, val loss = 1.4544, train loss = 1.3651)\n",
            "Saving model (epoch = 4328, val loss = 1.4542, train loss = 1.3164)\n",
            "Saving model (epoch = 4345, val loss = 1.4541, train loss = 1.1691)\n",
            "Saving model (epoch = 4350, val loss = 1.4541, train loss = 1.1888)\n",
            "Saving model (epoch = 4356, val loss = 1.4538, train loss = 1.4439)\n",
            "Saving model (epoch = 4366, val loss = 1.4538, train loss = 1.1747)\n",
            "Saving model (epoch = 4374, val loss = 1.4534, train loss = 1.2953)\n",
            "Saving model (epoch = 4392, val loss = 1.4533, train loss = 1.2589)\n",
            "Saving model (epoch = 4396, val loss = 1.4530, train loss = 1.2054)\n",
            "Saving model (epoch = 4420, val loss = 1.4529, train loss = 1.1799)\n",
            "Saving model (epoch = 4428, val loss = 1.4526, train loss = 1.1300)\n",
            "Saving model (epoch = 4431, val loss = 1.4525, train loss = 1.3115)\n",
            "Saving model (epoch = 4450, val loss = 1.4523, train loss = 1.2006)\n",
            "Saving model (epoch = 4468, val loss = 1.4517, train loss = 1.2818)\n",
            "Saving model (epoch = 4495, val loss = 1.4516, train loss = 1.2313)\n",
            "Saving model (epoch = 4497, val loss = 1.4515, train loss = 1.1190)\n",
            "Saving model (epoch = 4506, val loss = 1.4511, train loss = 1.2302)\n",
            "Saving model (epoch = 4521, val loss = 1.4511, train loss = 1.1291)\n",
            "Saving model (epoch = 4529, val loss = 1.4508, train loss = 1.2453)\n",
            "Saving model (epoch = 4551, val loss = 1.4508, train loss = 1.4380)\n",
            "Saving model (epoch = 4554, val loss = 1.4506, train loss = 1.4571)\n",
            "Saving model (epoch = 4567, val loss = 1.4505, train loss = 1.0846)\n",
            "Saving model (epoch = 4569, val loss = 1.4504, train loss = 1.3356)\n",
            "Saving model (epoch = 4576, val loss = 1.4503, train loss = 1.2773)\n",
            "Saving model (epoch = 4584, val loss = 1.4502, train loss = 1.1934)\n",
            "Saving model (epoch = 4595, val loss = 1.4499, train loss = 1.2148)\n",
            "Saving model (epoch = 4596, val loss = 1.4497, train loss = 1.3348)\n",
            "Saving model (epoch = 4624, val loss = 1.4497, train loss = 1.3817)\n",
            "Saving model (epoch = 4636, val loss = 1.4494, train loss = 1.1745)\n",
            "Saving model (epoch = 4637, val loss = 1.4494, train loss = 1.1839)\n",
            "Saving model (epoch = 4643, val loss = 1.4493, train loss = 1.2955)\n",
            "Saving model (epoch = 4649, val loss = 1.4493, train loss = 1.3372)\n",
            "Saving model (epoch = 4662, val loss = 1.4486, train loss = 1.2683)\n",
            "Saving model (epoch = 4692, val loss = 1.4485, train loss = 1.2534)\n",
            "Saving model (epoch = 4710, val loss = 1.4481, train loss = 1.1250)\n",
            "Saving model (epoch = 4723, val loss = 1.4479, train loss = 1.2228)\n",
            "Saving model (epoch = 4749, val loss = 1.4478, train loss = 1.1295)\n",
            "Saving model (epoch = 4753, val loss = 1.4478, train loss = 1.2193)\n",
            "Saving model (epoch = 4756, val loss = 1.4476, train loss = 1.4102)\n",
            "Saving model (epoch = 4767, val loss = 1.4473, train loss = 1.2782)\n",
            "Saving model (epoch = 4779, val loss = 1.4472, train loss = 1.1900)\n",
            "Saving model (epoch = 4790, val loss = 1.4471, train loss = 1.0984)\n",
            "Saving model (epoch = 4795, val loss = 1.4471, train loss = 1.1426)\n",
            "Saving model (epoch = 4796, val loss = 1.4470, train loss = 1.3146)\n",
            "Saving model (epoch = 4810, val loss = 1.4469, train loss = 1.3221)\n",
            "Saving model (epoch = 4819, val loss = 1.4468, train loss = 1.1754)\n",
            "Saving model (epoch = 4827, val loss = 1.4467, train loss = 1.3177)\n",
            "Saving model (epoch = 4830, val loss = 1.4467, train loss = 1.2804)\n",
            "Saving model (epoch = 4832, val loss = 1.4464, train loss = 1.1799)\n",
            "Saving model (epoch = 4851, val loss = 1.4463, train loss = 1.3839)\n",
            "Saving model (epoch = 4854, val loss = 1.4462, train loss = 1.1277)\n",
            "Saving model (epoch = 4868, val loss = 1.4462, train loss = 1.2818)\n",
            "Saving model (epoch = 4872, val loss = 1.4461, train loss = 1.1531)\n",
            "Saving model (epoch = 4883, val loss = 1.4459, train loss = 1.2402)\n",
            "Saving model (epoch = 4891, val loss = 1.4459, train loss = 1.1604)\n",
            "Saving model (epoch = 4892, val loss = 1.4459, train loss = 1.2118)\n",
            "Saving model (epoch = 4895, val loss = 1.4459, train loss = 1.2623)\n",
            "Saving model (epoch = 4897, val loss = 1.4458, train loss = 1.2469)\n",
            "Saving model (epoch = 4900, val loss = 1.4458, train loss = 1.3925)\n",
            "Saving model (epoch = 4906, val loss = 1.4458, train loss = 1.2351)\n",
            "Saving model (epoch = 4909, val loss = 1.4455, train loss = 1.2396)\n",
            "Saving model (epoch = 4912, val loss = 1.4453, train loss = 1.2789)\n",
            "Saving model (epoch = 4932, val loss = 1.4452, train loss = 1.2196)\n",
            "Saving model (epoch = 4940, val loss = 1.4448, train loss = 1.2054)\n",
            "Saving model (epoch = 4969, val loss = 1.4443, train loss = 1.2107)\n",
            "Saving model (epoch = 4984, val loss = 1.4442, train loss = 1.2381)\n",
            "Saving model (epoch = 5010, val loss = 1.4441, train loss = 1.2223)\n",
            "Saving model (epoch = 5011, val loss = 1.4439, train loss = 1.3035)\n",
            "Saving model (epoch = 5029, val loss = 1.4439, train loss = 1.2141)\n",
            "Saving model (epoch = 5036, val loss = 1.4436, train loss = 1.1769)\n",
            "Saving model (epoch = 5050, val loss = 1.4436, train loss = 1.3106)\n",
            "Saving model (epoch = 5060, val loss = 1.4433, train loss = 1.3115)\n",
            "Saving model (epoch = 5070, val loss = 1.4433, train loss = 1.2614)\n",
            "Saving model (epoch = 5072, val loss = 1.4432, train loss = 1.2109)\n",
            "Saving model (epoch = 5078, val loss = 1.4431, train loss = 1.3137)\n",
            "Saving model (epoch = 5091, val loss = 1.4430, train loss = 1.2661)\n",
            "Saving model (epoch = 5107, val loss = 1.4426, train loss = 1.1761)\n",
            "Saving model (epoch = 5148, val loss = 1.4423, train loss = 1.2011)\n",
            "Saving model (epoch = 5152, val loss = 1.4423, train loss = 1.1970)\n",
            "Saving model (epoch = 5163, val loss = 1.4420, train loss = 1.2351)\n",
            "Saving model (epoch = 5175, val loss = 1.4419, train loss = 1.2710)\n",
            "Saving model (epoch = 5181, val loss = 1.4419, train loss = 1.2184)\n",
            "Saving model (epoch = 5191, val loss = 1.4417, train loss = 1.1576)\n",
            "Saving model (epoch = 5205, val loss = 1.4417, train loss = 1.1729)\n",
            "Saving model (epoch = 5212, val loss = 1.4415, train loss = 1.4116)\n",
            "Saving model (epoch = 5227, val loss = 1.4414, train loss = 1.1307)\n",
            "Saving model (epoch = 5234, val loss = 1.4411, train loss = 1.1847)\n",
            "Saving model (epoch = 5249, val loss = 1.4410, train loss = 1.2268)\n",
            "Saving model (epoch = 5252, val loss = 1.4410, train loss = 1.3450)\n",
            "Saving model (epoch = 5264, val loss = 1.4408, train loss = 1.2225)\n",
            "Saving model (epoch = 5279, val loss = 1.4408, train loss = 1.4248)\n",
            "Saving model (epoch = 5295, val loss = 1.4407, train loss = 1.1620)\n",
            "Saving model (epoch = 5310, val loss = 1.4407, train loss = 1.3706)\n",
            "Saving model (epoch = 5313, val loss = 1.4402, train loss = 1.1327)\n",
            "Saving model (epoch = 5345, val loss = 1.4400, train loss = 1.2319)\n",
            "Saving model (epoch = 5347, val loss = 1.4400, train loss = 1.2095)\n",
            "Saving model (epoch = 5364, val loss = 1.4398, train loss = 1.1901)\n",
            "Saving model (epoch = 5393, val loss = 1.4397, train loss = 1.1834)\n",
            "Saving model (epoch = 5398, val loss = 1.4392, train loss = 1.2310)\n",
            "Saving model (epoch = 5438, val loss = 1.4391, train loss = 1.2838)\n",
            "Saving model (epoch = 5451, val loss = 1.4391, train loss = 1.1910)\n",
            "Saving model (epoch = 5453, val loss = 1.4387, train loss = 1.2229)\n",
            "Saving model (epoch = 5498, val loss = 1.4386, train loss = 1.3522)\n",
            "Saving model (epoch = 5503, val loss = 1.4385, train loss = 1.1453)\n",
            "Saving model (epoch = 5506, val loss = 1.4384, train loss = 1.2448)\n",
            "Saving model (epoch = 5513, val loss = 1.4381, train loss = 1.3525)\n",
            "Saving model (epoch = 5523, val loss = 1.4379, train loss = 1.2970)\n",
            "Saving model (epoch = 5552, val loss = 1.4378, train loss = 1.3265)\n",
            "Saving model (epoch = 5569, val loss = 1.4378, train loss = 1.3253)\n",
            "Saving model (epoch = 5576, val loss = 1.4377, train loss = 1.1056)\n",
            "Saving model (epoch = 5594, val loss = 1.4377, train loss = 1.1134)\n",
            "Saving model (epoch = 5598, val loss = 1.4374, train loss = 1.2656)\n",
            "Saving model (epoch = 5608, val loss = 1.4372, train loss = 1.2493)\n",
            "Saving model (epoch = 5630, val loss = 1.4372, train loss = 1.3662)\n",
            "Saving model (epoch = 5635, val loss = 1.4370, train loss = 1.3177)\n",
            "Saving model (epoch = 5644, val loss = 1.4369, train loss = 1.1195)\n",
            "Saving model (epoch = 5664, val loss = 1.4368, train loss = 1.2643)\n",
            "Saving model (epoch = 5673, val loss = 1.4367, train loss = 1.1926)\n",
            "Saving model (epoch = 5686, val loss = 1.4367, train loss = 1.3800)\n",
            "Saving model (epoch = 5696, val loss = 1.4364, train loss = 1.4058)\n",
            "Saving model (epoch = 5699, val loss = 1.4364, train loss = 1.3006)\n",
            "Saving model (epoch = 5702, val loss = 1.4362, train loss = 1.3746)\n",
            "Saving model (epoch = 5728, val loss = 1.4362, train loss = 1.2930)\n",
            "Saving model (epoch = 5736, val loss = 1.4361, train loss = 1.2196)\n",
            "Saving model (epoch = 5740, val loss = 1.4359, train loss = 1.1831)\n",
            "Saving model (epoch = 5757, val loss = 1.4359, train loss = 1.1163)\n",
            "Saving model (epoch = 5762, val loss = 1.4358, train loss = 1.0881)\n",
            "Saving model (epoch = 5765, val loss = 1.4355, train loss = 1.3101)\n",
            "Saving model (epoch = 5780, val loss = 1.4352, train loss = 1.4293)\n",
            "Saving model (epoch = 5813, val loss = 1.4352, train loss = 1.1173)\n",
            "Saving model (epoch = 5822, val loss = 1.4347, train loss = 1.1460)\n",
            "Saving model (epoch = 5859, val loss = 1.4346, train loss = 1.1187)\n",
            "Saving model (epoch = 5868, val loss = 1.4346, train loss = 1.4802)\n",
            "Saving model (epoch = 5875, val loss = 1.4345, train loss = 1.1428)\n",
            "Saving model (epoch = 5881, val loss = 1.4344, train loss = 1.1608)\n",
            "Saving model (epoch = 5882, val loss = 1.4343, train loss = 1.1313)\n",
            "Saving model (epoch = 5896, val loss = 1.4341, train loss = 1.3048)\n",
            "Saving model (epoch = 5907, val loss = 1.4340, train loss = 1.1872)\n",
            "Saving model (epoch = 5955, val loss = 1.4340, train loss = 1.1401)\n",
            "Saving model (epoch = 5958, val loss = 1.4339, train loss = 1.1657)\n",
            "Saving model (epoch = 5961, val loss = 1.4339, train loss = 1.2498)\n",
            "Saving model (epoch = 5962, val loss = 1.4339, train loss = 1.2860)\n",
            "Saving model (epoch = 5965, val loss = 1.4337, train loss = 1.2059)\n",
            "Saving model (epoch = 5969, val loss = 1.4336, train loss = 1.2258)\n",
            "Saving model (epoch = 5981, val loss = 1.4335, train loss = 1.1061)\n",
            "Saving model (epoch = 5994, val loss = 1.4335, train loss = 1.2743)\n",
            "Saving model (epoch = 6001, val loss = 1.4334, train loss = 1.2046)\n",
            "Saving model (epoch = 6012, val loss = 1.4333, train loss = 1.2620)\n",
            "Saving model (epoch = 6026, val loss = 1.4331, train loss = 1.1416)\n",
            "Saving model (epoch = 6027, val loss = 1.4330, train loss = 1.1722)\n",
            "Saving model (epoch = 6047, val loss = 1.4325, train loss = 1.1398)\n",
            "Saving model (epoch = 6099, val loss = 1.4325, train loss = 1.1349)\n",
            "Saving model (epoch = 6121, val loss = 1.4323, train loss = 1.3036)\n",
            "Saving model (epoch = 6124, val loss = 1.4321, train loss = 1.3183)\n",
            "Saving model (epoch = 6134, val loss = 1.4320, train loss = 1.2428)\n",
            "Saving model (epoch = 6135, val loss = 1.4319, train loss = 1.1053)\n",
            "Saving model (epoch = 6163, val loss = 1.4318, train loss = 1.1565)\n",
            "Saving model (epoch = 6173, val loss = 1.4318, train loss = 1.3165)\n",
            "Saving model (epoch = 6177, val loss = 1.4317, train loss = 1.4304)\n",
            "Saving model (epoch = 6193, val loss = 1.4316, train loss = 1.3133)\n",
            "Saving model (epoch = 6197, val loss = 1.4316, train loss = 1.0913)\n",
            "Saving model (epoch = 6200, val loss = 1.4315, train loss = 1.1763)\n",
            "Saving model (epoch = 6210, val loss = 1.4314, train loss = 1.1838)\n",
            "Saving model (epoch = 6218, val loss = 1.4313, train loss = 1.2189)\n",
            "Saving model (epoch = 6232, val loss = 1.4312, train loss = 1.0747)\n",
            "Saving model (epoch = 6237, val loss = 1.4312, train loss = 1.2192)\n",
            "Saving model (epoch = 6242, val loss = 1.4308, train loss = 1.2319)\n",
            "Saving model (epoch = 6307, val loss = 1.4307, train loss = 1.2207)\n",
            "Saving model (epoch = 6310, val loss = 1.4306, train loss = 1.3800)\n",
            "Saving model (epoch = 6336, val loss = 1.4306, train loss = 1.3382)\n",
            "Saving model (epoch = 6338, val loss = 1.4304, train loss = 1.0212)\n",
            "Saving model (epoch = 6359, val loss = 1.4302, train loss = 1.2443)\n",
            "Saving model (epoch = 6362, val loss = 1.4301, train loss = 1.2003)\n",
            "Saving model (epoch = 6372, val loss = 1.4300, train loss = 1.3336)\n",
            "Saving model (epoch = 6398, val loss = 1.4298, train loss = 1.2110)\n",
            "Saving model (epoch = 6417, val loss = 1.4298, train loss = 1.2607)\n",
            "Saving model (epoch = 6423, val loss = 1.4297, train loss = 1.1562)\n",
            "Saving model (epoch = 6442, val loss = 1.4296, train loss = 1.2383)\n",
            "Saving model (epoch = 6458, val loss = 1.4295, train loss = 1.0413)\n",
            "Saving model (epoch = 6471, val loss = 1.4293, train loss = 1.2433)\n",
            "Saving model (epoch = 6502, val loss = 1.4292, train loss = 1.3013)\n",
            "Saving model (epoch = 6512, val loss = 1.4291, train loss = 1.3020)\n",
            "Saving model (epoch = 6521, val loss = 1.4291, train loss = 1.1451)\n",
            "Saving model (epoch = 6523, val loss = 1.4290, train loss = 1.2212)\n",
            "Saving model (epoch = 6530, val loss = 1.4290, train loss = 1.1181)\n",
            "Saving model (epoch = 6535, val loss = 1.4286, train loss = 1.2432)\n",
            "Saving model (epoch = 6576, val loss = 1.4285, train loss = 1.1340)\n",
            "Saving model (epoch = 6578, val loss = 1.4284, train loss = 1.1603)\n",
            "Saving model (epoch = 6593, val loss = 1.4283, train loss = 1.1282)\n",
            "Saving model (epoch = 6604, val loss = 1.4283, train loss = 1.1241)\n",
            "Saving model (epoch = 6616, val loss = 1.4281, train loss = 1.3271)\n",
            "Saving model (epoch = 6644, val loss = 1.4281, train loss = 1.3114)\n",
            "Saving model (epoch = 6648, val loss = 1.4279, train loss = 1.3153)\n",
            "Saving model (epoch = 6653, val loss = 1.4278, train loss = 1.1522)\n",
            "Saving model (epoch = 6675, val loss = 1.4278, train loss = 1.2519)\n",
            "Saving model (epoch = 6689, val loss = 1.4273, train loss = 1.2847)\n",
            "Saving model (epoch = 6723, val loss = 1.4271, train loss = 1.4304)\n",
            "Saving model (epoch = 6766, val loss = 1.4271, train loss = 1.0805)\n",
            "Saving model (epoch = 6799, val loss = 1.4269, train loss = 1.1928)\n",
            "Saving model (epoch = 6822, val loss = 1.4267, train loss = 1.2119)\n",
            "Saving model (epoch = 6825, val loss = 1.4267, train loss = 1.2556)\n",
            "Saving model (epoch = 6839, val loss = 1.4264, train loss = 1.3174)\n",
            "Saving model (epoch = 6845, val loss = 1.4263, train loss = 1.1641)\n",
            "Saving model (epoch = 6863, val loss = 1.4263, train loss = 1.1683)\n",
            "Saving model (epoch = 6867, val loss = 1.4262, train loss = 1.4319)\n",
            "Saving model (epoch = 6889, val loss = 1.4261, train loss = 1.2432)\n",
            "Saving model (epoch = 6905, val loss = 1.4259, train loss = 1.2926)\n",
            "Saving model (epoch = 6951, val loss = 1.4256, train loss = 1.3464)\n",
            "Saving model (epoch = 6976, val loss = 1.4255, train loss = 1.3635)\n",
            "Saving model (epoch = 6989, val loss = 1.4255, train loss = 1.1773)\n",
            "Saving model (epoch = 7011, val loss = 1.4253, train loss = 1.2666)\n",
            "Saving model (epoch = 7053, val loss = 1.4251, train loss = 1.2596)\n",
            "Saving model (epoch = 7068, val loss = 1.4250, train loss = 1.1546)\n",
            "Saving model (epoch = 7076, val loss = 1.4250, train loss = 1.3691)\n",
            "Saving model (epoch = 7089, val loss = 1.4249, train loss = 1.2201)\n",
            "Saving model (epoch = 7091, val loss = 1.4248, train loss = 1.2121)\n",
            "Saving model (epoch = 7098, val loss = 1.4248, train loss = 1.2601)\n",
            "Saving model (epoch = 7105, val loss = 1.4248, train loss = 1.3858)\n",
            "Saving model (epoch = 7137, val loss = 1.4243, train loss = 1.2570)\n",
            "Saving model (epoch = 7171, val loss = 1.4242, train loss = 1.1390)\n",
            "Saving model (epoch = 7172, val loss = 1.4242, train loss = 1.2472)\n",
            "Saving model (epoch = 7211, val loss = 1.4242, train loss = 1.1998)\n",
            "Saving model (epoch = 7219, val loss = 1.4241, train loss = 1.2969)\n",
            "Saving model (epoch = 7241, val loss = 1.4241, train loss = 1.1487)\n",
            "Saving model (epoch = 7244, val loss = 1.4241, train loss = 1.2878)\n",
            "Saving model (epoch = 7251, val loss = 1.4240, train loss = 1.1747)\n",
            "Saving model (epoch = 7258, val loss = 1.4239, train loss = 1.3757)\n",
            "Saving model (epoch = 7289, val loss = 1.4236, train loss = 1.2706)\n",
            "Saving model (epoch = 7321, val loss = 1.4233, train loss = 1.0651)\n",
            "Saving model (epoch = 7374, val loss = 1.4232, train loss = 1.1560)\n",
            "Saving model (epoch = 7392, val loss = 1.4229, train loss = 1.2940)\n",
            "Saving model (epoch = 7405, val loss = 1.4228, train loss = 1.2908)\n",
            "Saving model (epoch = 7434, val loss = 1.4228, train loss = 1.1233)\n",
            "Saving model (epoch = 7436, val loss = 1.4227, train loss = 1.1671)\n",
            "Saving model (epoch = 7465, val loss = 1.4227, train loss = 1.2483)\n",
            "Saving model (epoch = 7478, val loss = 1.4227, train loss = 1.1655)\n",
            "Saving model (epoch = 7480, val loss = 1.4226, train loss = 1.1967)\n",
            "Saving model (epoch = 7489, val loss = 1.4226, train loss = 1.2366)\n",
            "Saving model (epoch = 7517, val loss = 1.4225, train loss = 1.2839)\n",
            "Saving model (epoch = 7534, val loss = 1.4224, train loss = 1.0436)\n",
            "Saving model (epoch = 7542, val loss = 1.4224, train loss = 1.1901)\n",
            "Saving model (epoch = 7555, val loss = 1.4223, train loss = 1.2947)\n",
            "Saving model (epoch = 7560, val loss = 1.4220, train loss = 1.2606)\n",
            "Saving model (epoch = 7603, val loss = 1.4220, train loss = 1.2879)\n",
            "Saving model (epoch = 7615, val loss = 1.4219, train loss = 1.2441)\n",
            "Saving model (epoch = 7620, val loss = 1.4217, train loss = 1.4018)\n",
            "Saving model (epoch = 7640, val loss = 1.4214, train loss = 1.1974)\n",
            "Saving model (epoch = 7726, val loss = 1.4211, train loss = 1.2236)\n",
            "Saving model (epoch = 7778, val loss = 1.4210, train loss = 1.2712)\n",
            "Saving model (epoch = 7835, val loss = 1.4209, train loss = 1.2183)\n",
            "Saving model (epoch = 7860, val loss = 1.4207, train loss = 1.1565)\n",
            "Saving model (epoch = 7872, val loss = 1.4201, train loss = 1.2936)\n",
            "Finished training after 8000 epochs\n",
            "Final: (epoch = 8001, val loss = 1.4201, train loss = 1.3083)\n",
            "Final: (epoch = 8001, val loss = 1.0458, train loss = 0.9340)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hsNO9nnXQBvP",
        "outputId": "b6305449-0ef8-45d2-d7b7-7bd61b88cf3b"
      },
      "source": [
        "plot_learning_curve(model_loss_record, title='deep model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3iZTVn5WQFpX",
        "outputId": "be0a06b7-ac4d-46ff-9824-2121e604a0bd"
      },
      "source": [
        "del model\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQikz3IPiyPf"
      },
      "source": [
        "# **Testing**\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cTuQjQQOon",
        "outputId": "7298008e-9a5d-4f57-d0f9-a765946871c8"
      },
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    print('Saving results to {}'.format(file))\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'tested_positive'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i, p])\n",
        "\n",
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to pred.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfrVxqJanGpE"
      },
      "source": [
        "# **Hints**\n",
        "\n",
        "## **Simple Baseline**\n",
        "* Run sample code\n",
        "\n",
        "## **Medium Baseline**\n",
        "* Feature selection: 40 states + 2 `tested_positive` (`TODO` in dataset)\n",
        "\n",
        "## **Strong Baseline**\n",
        "* Feature selection (what other features are useful?)\n",
        "* DNN architecture (layers? dimension? activation function?)\n",
        "* Training (mini-batch? optimizer? learning rate?)\n",
        "* L2 regularization\n",
        "* There are some mistakes in the sample code, can you find them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tmCwXgpot3t"
      },
      "source": [
        "# **Reference**\n",
        "\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"
      ]
    }
  ]
}